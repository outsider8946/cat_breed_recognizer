{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "15_pkL6I6k_Ykjl0Sy086x50M17A9P_BI",
      "authorship_tag": "ABX9TyNnxU7VfNYH/8NPu+U48MrJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/outsider8946/cat_breed_recognizer/blob/main/model/train_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IC2jjdXb17jk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_TO_DATA = '/content/drive/MyDrive/Colab Notebooks/cat_breed_recognizer/data/'\n",
        "cat_breeds = ['bengal', 'maine_coon', 'ragdoll', 'oriental_shorthair', 'british_shorthair','siamese']"
      ],
      "metadata": {
        "id": "kUDloEn5-mr1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "device =  torch.device(dev)\n",
        "dev"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "rhbmytoa9Re1",
        "outputId": "4770d2f0-454e-43f4-b173-6d076481ed51"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = []\n",
        "y = []\n",
        "for breed in cat_breeds:\n",
        "  print(breed)\n",
        "  if os.path.exists(PATH_TO_DATA+breed+f'/{breed}.csv'):\n",
        "    df = pd.read_csv(PATH_TO_DATA+breed+f'/{breed}.csv',on_bad_lines='skip')\n",
        "    for i in range(len(df)):\n",
        "      img_name = df.loc[i,'image_name']\n",
        "      img_mat = cv2.imread(PATH_TO_DATA+breed+f'/{img_name}')\n",
        "      img_mat = cv2.resize(img_mat,(32,32))\n",
        "      img_mat= img_mat/255\n",
        "      X.append(img_mat)\n",
        "      y.append(df.loc[i, ['label_name','norm_bbox_x', 'norm_bbox_y', 'norm_bbox_width', 'norm_bbox_height','image_width','image_height']].to_numpy().tolist())\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "X.shape, y.shape"
      ],
      "metadata": {
        "id": "2AJr4jzOOJ3q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaf041d8-facb-461b-b2a0-b8e1f33e3db0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bengal\n",
            "maine_coon\n",
            "ragdoll\n",
            "oriental_shorthair\n",
            "british_shorthair\n",
            "siamese\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1155, 32, 32, 3), (1155, 7))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.load(PATH_TO_DATA+'X.npy')\n",
        "y = np.load(PATH_TO_DATA+'y.npy')"
      ],
      "metadata": {
        "id": "8OncBKyNQl-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(data=y[:,0])"
      ],
      "metadata": {
        "id": "sUK_4X8Og6Do",
        "outputId": "a764b4fc-b6aa-43d7-b0d7-6ddec3cd77b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: ylabel='Count'>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y,shuffle=True,random_state=42)\n",
        "\n",
        "X_train = torch.tensor(X_train).float().to(device)\n",
        "y_train = torch.tensor(y_train).float().to(device)\n",
        "X_test = torch.tensor(X_test).float().to(device)\n",
        "y_test = torch.tensor(y_test).float().to(device)\n",
        "\n",
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAO-2e57dF7r",
        "outputId": "1bd547d4-133d-4fc0-8a8c-796736fe8ede"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([866, 32, 32, 3]),\n",
              " torch.Size([866, 7]),\n",
              " torch.Size([289, 32, 32, 3]),\n",
              " torch.Size([289, 7]))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CatDataset(Dataset):\n",
        "  def __init__(self,X,y):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "\n",
        "  def __getitem__(self,i):\n",
        "    return self.X[i], self.y[i]\n",
        "\n",
        "train_dataset = CatDataset(X_train,y_train)\n",
        "test_dataset = CatDataset(X_test,y_test)"
      ],
      "metadata": {
        "id": "Fc_em8LUGgUn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "train_dataloader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset,batch_size=batch_size,shuffle=True)"
      ],
      "metadata": {
        "id": "0aq7q5pS7gHD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CatModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.regressor = nn.Sequential(\n",
        "        nn.Conv2d(3, 32, kernel_size=(3,3), stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=(2, 2)),\n",
        "        nn.Dropout(0.3),\n",
        "\n",
        "        nn.Conv2d(32,32,kernel_size=(3,3),stride=1,padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=(2, 2)),\n",
        "        nn.Flatten(),\n",
        "\n",
        "        nn.Linear(8*8*32, 8*32),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Linear(8*32, 4*32),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Linear(4*32, 32),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Linear(32, 4),\n",
        "     )\n",
        "\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Conv2d(3, 32, kernel_size=(3,3), stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.MaxPool2d(kernel_size=(2, 2)),\n",
        "        nn.Dropout(0.3),\n",
        "\n",
        "        nn.Conv2d(32,32,kernel_size=(3,3),stride=1,padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.MaxPool2d(kernel_size=(2, 2)),\n",
        "        nn.Flatten(),\n",
        "\n",
        "        nn.Linear(8*8*32, 4*32),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Linear(4*32, 32),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Linear(32, 6),\n",
        "        nn.Softmax()\n",
        "      )\n",
        "\n",
        "  def forward(self,x):\n",
        "    # reshape\n",
        "    x = x.view(x.shape[0], 3, 32,32)\n",
        "    return self.regressor(x), self.classifier(x)\n"
      ],
      "metadata": {
        "id": "NJAL6-Tp2AzE"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CatModel()\n",
        "model.to(device)\n",
        "bbox_loss = nn.MSELoss()\n",
        "label_loss = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "n_epochs = 300\n",
        "for epoch in range(n_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_dataloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        input, targets = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        #convert targets to correct dimension\n",
        "        labels = np.array([[0]*6]*targets.shape[0],dtype=np.float64)\n",
        "        for batch in range(targets.shape[0]):\n",
        "          labels[batch,int(targets[batch,0].cpu())] = 1\n",
        "        labels = torch.tensor(labels).float().to(device)\n",
        "        bounds = targets[:,1:4+1]\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        output = model(input)\n",
        "        regressor_loss = bbox_loss(output[0], bounds)\n",
        "\n",
        "        classifier_loss = label_loss(output[1],labels)\n",
        "        total_loss = classifier_loss + regressor_loss\n",
        "\n",
        "        #total_loss.backward()\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += total_loss.item()\n",
        "\n",
        "        if i % 10 == 9:    # print every 10 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] running loss:{running_loss:.5f},  box loss: {regressor_loss:.5f}, class loss:{classifier_loss:.5f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "id": "ShwYn06Z6_ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "783e1ab0-7682-4067-aa53-429ef1bd32c5"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,    10] running loss:18.82077,  box loss: 0.04584, class loss:1.74617\n",
            "[1,    20] running loss:17.81545,  box loss: 0.04191, class loss:1.70450\n",
            "[2,    10] running loss:17.06927,  box loss: 0.03997, class loss:1.47088\n",
            "[2,    20] running loss:17.24190,  box loss: 0.03187, class loss:1.72719\n",
            "[3,    10] running loss:16.95101,  box loss: 0.04036, class loss:1.67056\n",
            "[3,    20] running loss:16.95713,  box loss: 0.03525, class loss:1.77414\n",
            "[4,    10] running loss:16.24142,  box loss: 0.03205, class loss:1.59181\n",
            "[4,    20] running loss:16.47968,  box loss: 0.03464, class loss:1.59514\n",
            "[5,    10] running loss:16.08708,  box loss: 0.02505, class loss:1.53764\n",
            "[5,    20] running loss:15.84888,  box loss: 0.02509, class loss:1.56156\n",
            "[6,    10] running loss:15.60624,  box loss: 0.02892, class loss:1.44890\n",
            "[6,    20] running loss:15.80618,  box loss: 0.02688, class loss:1.69468\n",
            "[7,    10] running loss:15.07882,  box loss: 0.03387, class loss:1.44924\n",
            "[7,    20] running loss:15.16236,  box loss: 0.03832, class loss:1.52068\n",
            "[8,    10] running loss:15.35665,  box loss: 0.02634, class loss:1.61581\n",
            "[8,    20] running loss:15.34355,  box loss: 0.03693, class loss:1.65845\n",
            "[9,    10] running loss:15.01663,  box loss: 0.02776, class loss:1.50701\n",
            "[9,    20] running loss:14.99231,  box loss: 0.03390, class loss:1.44025\n",
            "[10,    10] running loss:14.58946,  box loss: 0.04106, class loss:1.40044\n",
            "[10,    20] running loss:14.42248,  box loss: 0.03199, class loss:1.39799\n",
            "[11,    10] running loss:14.31711,  box loss: 0.02506, class loss:1.37933\n",
            "[11,    20] running loss:14.28947,  box loss: 0.03853, class loss:1.29578\n",
            "[12,    10] running loss:14.01788,  box loss: 0.03977, class loss:1.38681\n",
            "[12,    20] running loss:14.18799,  box loss: 0.03037, class loss:1.38877\n",
            "[13,    10] running loss:14.34187,  box loss: 0.03546, class loss:1.25213\n",
            "[13,    20] running loss:13.76380,  box loss: 0.03776, class loss:1.35172\n",
            "[14,    10] running loss:13.83420,  box loss: 0.03408, class loss:1.24640\n",
            "[14,    20] running loss:14.05505,  box loss: 0.02851, class loss:1.40730\n",
            "[15,    10] running loss:13.91278,  box loss: 0.03259, class loss:1.26187\n",
            "[15,    20] running loss:13.73771,  box loss: 0.02878, class loss:1.24194\n",
            "[16,    10] running loss:13.86424,  box loss: 0.04761, class loss:1.27410\n",
            "[16,    20] running loss:13.20391,  box loss: 0.03344, class loss:1.24673\n",
            "[17,    10] running loss:13.74968,  box loss: 0.03658, class loss:1.36248\n",
            "[17,    20] running loss:12.99750,  box loss: 0.02437, class loss:1.32646\n",
            "[18,    10] running loss:13.15515,  box loss: 0.02426, class loss:1.27162\n",
            "[18,    20] running loss:13.34724,  box loss: 0.01696, class loss:1.32353\n",
            "[19,    10] running loss:13.11824,  box loss: 0.02885, class loss:1.32503\n",
            "[19,    20] running loss:13.25539,  box loss: 0.03141, class loss:1.25488\n",
            "[20,    10] running loss:13.12354,  box loss: 0.02265, class loss:1.25900\n",
            "[20,    20] running loss:13.11537,  box loss: 0.02786, class loss:1.26536\n",
            "[21,    10] running loss:13.00449,  box loss: 0.03265, class loss:1.36214\n",
            "[21,    20] running loss:13.04637,  box loss: 0.02564, class loss:1.22312\n",
            "[22,    10] running loss:13.31481,  box loss: 0.02939, class loss:1.26500\n",
            "[22,    20] running loss:12.68754,  box loss: 0.01779, class loss:1.25730\n",
            "[23,    10] running loss:13.83950,  box loss: 0.02498, class loss:1.21337\n",
            "[23,    20] running loss:13.34549,  box loss: 0.02995, class loss:1.34106\n",
            "[24,    10] running loss:13.08325,  box loss: 0.02994, class loss:1.39378\n",
            "[24,    20] running loss:13.59655,  box loss: 0.02108, class loss:1.33760\n",
            "[25,    10] running loss:12.90808,  box loss: 0.02095, class loss:1.14132\n",
            "[25,    20] running loss:12.91211,  box loss: 0.01561, class loss:1.32071\n",
            "[26,    10] running loss:12.84804,  box loss: 0.02908, class loss:1.29143\n",
            "[26,    20] running loss:12.80441,  box loss: 0.01697, class loss:1.17026\n",
            "[27,    10] running loss:13.03481,  box loss: 0.02270, class loss:1.28843\n",
            "[27,    20] running loss:12.76661,  box loss: 0.01946, class loss:1.27112\n",
            "[28,    10] running loss:12.95467,  box loss: 0.01995, class loss:1.23481\n",
            "[28,    20] running loss:12.68970,  box loss: 0.02192, class loss:1.23707\n",
            "[29,    10] running loss:12.88042,  box loss: 0.02079, class loss:1.26193\n",
            "[29,    20] running loss:12.86881,  box loss: 0.01843, class loss:1.20990\n",
            "[30,    10] running loss:13.00842,  box loss: 0.02628, class loss:1.23371\n",
            "[30,    20] running loss:12.27110,  box loss: 0.02025, class loss:1.24711\n",
            "[31,    10] running loss:12.81437,  box loss: 0.01853, class loss:1.22546\n",
            "[31,    20] running loss:12.69411,  box loss: 0.01950, class loss:1.20428\n",
            "[32,    10] running loss:12.65192,  box loss: 0.01873, class loss:1.20580\n",
            "[32,    20] running loss:12.58009,  box loss: 0.01620, class loss:1.41091\n",
            "[33,    10] running loss:12.50677,  box loss: 0.01547, class loss:1.28798\n",
            "[33,    20] running loss:12.88972,  box loss: 0.02252, class loss:1.32625\n",
            "[34,    10] running loss:13.26535,  box loss: 0.01652, class loss:1.37350\n",
            "[34,    20] running loss:13.12287,  box loss: 0.01537, class loss:1.28167\n",
            "[35,    10] running loss:13.08789,  box loss: 0.02181, class loss:1.21010\n",
            "[35,    20] running loss:12.45226,  box loss: 0.01909, class loss:1.27694\n",
            "[36,    10] running loss:12.88719,  box loss: 0.01205, class loss:1.31140\n",
            "[36,    20] running loss:13.54853,  box loss: 0.01693, class loss:1.28582\n",
            "[37,    10] running loss:13.29215,  box loss: 0.01987, class loss:1.27494\n",
            "[37,    20] running loss:12.36629,  box loss: 0.01675, class loss:1.18139\n",
            "[38,    10] running loss:13.19185,  box loss: 0.01929, class loss:1.28974\n",
            "[38,    20] running loss:13.04601,  box loss: 0.01978, class loss:1.23243\n",
            "[39,    10] running loss:12.19317,  box loss: 0.01808, class loss:1.11074\n",
            "[39,    20] running loss:12.47978,  box loss: 0.01900, class loss:1.35364\n",
            "[40,    10] running loss:12.24101,  box loss: 0.01182, class loss:1.26868\n",
            "[40,    20] running loss:12.04894,  box loss: 0.01710, class loss:1.20384\n",
            "[41,    10] running loss:11.78152,  box loss: 0.01647, class loss:1.22624\n",
            "[41,    20] running loss:12.00967,  box loss: 0.01428, class loss:1.06706\n",
            "[42,    10] running loss:11.78811,  box loss: 0.01567, class loss:1.14620\n",
            "[42,    20] running loss:11.82999,  box loss: 0.01581, class loss:1.14699\n",
            "[43,    10] running loss:11.88652,  box loss: 0.01206, class loss:1.16944\n",
            "[43,    20] running loss:11.86381,  box loss: 0.01853, class loss:1.16187\n",
            "[44,    10] running loss:11.99742,  box loss: 0.01666, class loss:1.22857\n",
            "[44,    20] running loss:12.20129,  box loss: 0.01510, class loss:1.14008\n",
            "[45,    10] running loss:11.73822,  box loss: 0.01621, class loss:1.17232\n",
            "[45,    20] running loss:11.96137,  box loss: 0.01170, class loss:1.21951\n",
            "[46,    10] running loss:11.71488,  box loss: 0.01868, class loss:1.10554\n",
            "[46,    20] running loss:11.78686,  box loss: 0.01548, class loss:1.10378\n",
            "[47,    10] running loss:11.35044,  box loss: 0.01342, class loss:1.10957\n",
            "[47,    20] running loss:12.05964,  box loss: 0.01405, class loss:1.19981\n",
            "[48,    10] running loss:11.62730,  box loss: 0.01530, class loss:1.04403\n",
            "[48,    20] running loss:11.63829,  box loss: 0.01204, class loss:1.19645\n",
            "[49,    10] running loss:11.67927,  box loss: 0.01129, class loss:1.10552\n",
            "[49,    20] running loss:11.67477,  box loss: 0.01399, class loss:1.13555\n",
            "[50,    10] running loss:11.46302,  box loss: 0.01620, class loss:1.14424\n",
            "[50,    20] running loss:11.59001,  box loss: 0.01522, class loss:1.13798\n",
            "[51,    10] running loss:11.40234,  box loss: 0.01108, class loss:1.13008\n",
            "[51,    20] running loss:11.47915,  box loss: 0.01370, class loss:1.08321\n",
            "[52,    10] running loss:11.31868,  box loss: 0.00874, class loss:1.20222\n",
            "[52,    20] running loss:11.71441,  box loss: 0.01254, class loss:1.22488\n",
            "[53,    10] running loss:11.42079,  box loss: 0.01727, class loss:1.04737\n",
            "[53,    20] running loss:11.56691,  box loss: 0.01135, class loss:1.13986\n",
            "[54,    10] running loss:11.47980,  box loss: 0.01146, class loss:1.10717\n",
            "[54,    20] running loss:11.64619,  box loss: 0.01191, class loss:1.16887\n",
            "[55,    10] running loss:11.31235,  box loss: 0.01024, class loss:1.06191\n",
            "[55,    20] running loss:11.43654,  box loss: 0.01101, class loss:1.10488\n",
            "[56,    10] running loss:11.51186,  box loss: 0.01176, class loss:1.16888\n",
            "[56,    20] running loss:11.39774,  box loss: 0.01556, class loss:1.13301\n",
            "[57,    10] running loss:11.41857,  box loss: 0.01371, class loss:1.18301\n",
            "[57,    20] running loss:11.44566,  box loss: 0.01557, class loss:1.10824\n",
            "[58,    10] running loss:11.30565,  box loss: 0.00955, class loss:1.07733\n",
            "[58,    20] running loss:11.33398,  box loss: 0.00855, class loss:1.06848\n",
            "[59,    10] running loss:11.35096,  box loss: 0.01342, class loss:1.10933\n",
            "[59,    20] running loss:11.32676,  box loss: 0.00983, class loss:1.16875\n",
            "[60,    10] running loss:11.22443,  box loss: 0.01291, class loss:1.07581\n",
            "[60,    20] running loss:11.29457,  box loss: 0.01001, class loss:1.10567\n",
            "[61,    10] running loss:11.62870,  box loss: 0.01490, class loss:1.21792\n",
            "[61,    20] running loss:11.48858,  box loss: 0.01474, class loss:1.17423\n",
            "[62,    10] running loss:11.58876,  box loss: 0.00976, class loss:1.15959\n",
            "[62,    20] running loss:11.37328,  box loss: 0.00700, class loss:1.07628\n",
            "[63,    10] running loss:11.35761,  box loss: 0.00964, class loss:1.08134\n",
            "[63,    20] running loss:11.39997,  box loss: 0.01459, class loss:1.14693\n",
            "[64,    10] running loss:11.28930,  box loss: 0.00869, class loss:1.13763\n",
            "[64,    20] running loss:11.23170,  box loss: 0.01063, class loss:1.05688\n",
            "[65,    10] running loss:11.18177,  box loss: 0.00639, class loss:1.13566\n",
            "[65,    20] running loss:10.96684,  box loss: 0.01473, class loss:1.04546\n",
            "[66,    10] running loss:11.02694,  box loss: 0.01418, class loss:1.09254\n",
            "[66,    20] running loss:11.15616,  box loss: 0.01203, class loss:1.13676\n",
            "[67,    10] running loss:11.06870,  box loss: 0.01076, class loss:1.04364\n",
            "[67,    20] running loss:11.12426,  box loss: 0.01023, class loss:1.07402\n",
            "[68,    10] running loss:11.12957,  box loss: 0.00795, class loss:1.12458\n",
            "[68,    20] running loss:10.95055,  box loss: 0.01011, class loss:1.10703\n",
            "[69,    10] running loss:10.87593,  box loss: 0.01117, class loss:1.10280\n",
            "[69,    20] running loss:11.15278,  box loss: 0.00727, class loss:1.11974\n",
            "[70,    10] running loss:11.24562,  box loss: 0.01039, class loss:1.13678\n",
            "[70,    20] running loss:11.03568,  box loss: 0.01017, class loss:1.10909\n",
            "[71,    10] running loss:11.17530,  box loss: 0.01567, class loss:1.10705\n",
            "[71,    20] running loss:10.98310,  box loss: 0.00616, class loss:1.10733\n",
            "[72,    10] running loss:11.07546,  box loss: 0.00602, class loss:1.04374\n",
            "[72,    20] running loss:11.02175,  box loss: 0.00831, class loss:1.08301\n",
            "[73,    10] running loss:10.98834,  box loss: 0.01195, class loss:1.13554\n",
            "[73,    20] running loss:11.17614,  box loss: 0.01438, class loss:1.16745\n",
            "[74,    10] running loss:10.91507,  box loss: 0.00924, class loss:1.07633\n",
            "[74,    20] running loss:11.11961,  box loss: 0.00954, class loss:1.09001\n",
            "[75,    10] running loss:10.94692,  box loss: 0.00903, class loss:1.07327\n",
            "[75,    20] running loss:11.21839,  box loss: 0.01015, class loss:1.10759\n",
            "[76,    10] running loss:11.28584,  box loss: 0.01119, class loss:1.10841\n",
            "[76,    20] running loss:10.94389,  box loss: 0.01289, class loss:1.07484\n",
            "[77,    10] running loss:11.20849,  box loss: 0.00744, class loss:1.08643\n",
            "[77,    20] running loss:10.90088,  box loss: 0.01394, class loss:1.10427\n",
            "[78,    10] running loss:11.02018,  box loss: 0.01150, class loss:1.04366\n",
            "[78,    20] running loss:10.90945,  box loss: 0.00899, class loss:1.04387\n",
            "[79,    10] running loss:10.98734,  box loss: 0.00997, class loss:1.10540\n",
            "[79,    20] running loss:11.04819,  box loss: 0.01078, class loss:1.04360\n",
            "[80,    10] running loss:10.91184,  box loss: 0.00771, class loss:1.04457\n",
            "[80,    20] running loss:11.08908,  box loss: 0.01348, class loss:1.19587\n",
            "[81,    10] running loss:11.01416,  box loss: 0.00807, class loss:1.04517\n",
            "[81,    20] running loss:11.10057,  box loss: 0.00965, class loss:1.10586\n",
            "[82,    10] running loss:10.99179,  box loss: 0.01029, class loss:1.11397\n",
            "[82,    20] running loss:11.00064,  box loss: 0.01228, class loss:1.16545\n",
            "[83,    10] running loss:11.07271,  box loss: 0.01267, class loss:1.07472\n",
            "[83,    20] running loss:11.00738,  box loss: 0.00872, class loss:1.07550\n",
            "[84,    10] running loss:10.96264,  box loss: 0.00677, class loss:1.04419\n",
            "[84,    20] running loss:11.03909,  box loss: 0.00806, class loss:1.08933\n",
            "[85,    10] running loss:10.91127,  box loss: 0.00417, class loss:1.07613\n",
            "[85,    20] running loss:10.95445,  box loss: 0.01065, class loss:1.10619\n",
            "[86,    10] running loss:10.96744,  box loss: 0.00971, class loss:1.04381\n",
            "[86,    20] running loss:11.03832,  box loss: 0.01063, class loss:1.07273\n",
            "[87,    10] running loss:10.97347,  box loss: 0.00693, class loss:1.10632\n",
            "[87,    20] running loss:11.08191,  box loss: 0.00612, class loss:1.04421\n",
            "[88,    10] running loss:10.85377,  box loss: 0.01049, class loss:1.07360\n",
            "[88,    20] running loss:11.04022,  box loss: 0.00968, class loss:1.07531\n",
            "[89,    10] running loss:10.92307,  box loss: 0.00645, class loss:1.07503\n",
            "[89,    20] running loss:10.94082,  box loss: 0.01004, class loss:1.11302\n",
            "[90,    10] running loss:10.98669,  box loss: 0.00897, class loss:1.05145\n",
            "[90,    20] running loss:10.95284,  box loss: 0.00578, class loss:1.14347\n",
            "[91,    10] running loss:10.85348,  box loss: 0.01213, class loss:1.11355\n",
            "[91,    20] running loss:10.94057,  box loss: 0.01042, class loss:1.10438\n",
            "[92,    10] running loss:10.88057,  box loss: 0.01022, class loss:1.07583\n",
            "[92,    20] running loss:10.90538,  box loss: 0.01144, class loss:1.07328\n",
            "[93,    10] running loss:10.78952,  box loss: 0.00452, class loss:1.04361\n",
            "[93,    20] running loss:11.02766,  box loss: 0.00505, class loss:1.10619\n",
            "[94,    10] running loss:10.80580,  box loss: 0.01132, class loss:1.07499\n",
            "[94,    20] running loss:11.04350,  box loss: 0.00676, class loss:1.10539\n",
            "[95,    10] running loss:11.08145,  box loss: 0.00979, class loss:1.04368\n",
            "[95,    20] running loss:10.88186,  box loss: 0.00903, class loss:1.04454\n",
            "[96,    10] running loss:10.91705,  box loss: 0.00839, class loss:1.07460\n",
            "[96,    20] running loss:10.88829,  box loss: 0.00649, class loss:1.07334\n",
            "[97,    10] running loss:11.02359,  box loss: 0.00854, class loss:1.07519\n",
            "[97,    20] running loss:11.05410,  box loss: 0.00983, class loss:1.07500\n",
            "[98,    10] running loss:11.38210,  box loss: 0.01004, class loss:1.18817\n",
            "[98,    20] running loss:11.35504,  box loss: 0.00958, class loss:1.13597\n",
            "[99,    10] running loss:11.13335,  box loss: 0.00416, class loss:1.14101\n",
            "[99,    20] running loss:11.01000,  box loss: 0.00612, class loss:1.07629\n",
            "[100,    10] running loss:10.88592,  box loss: 0.00487, class loss:1.07548\n",
            "[100,    20] running loss:10.94294,  box loss: 0.01361, class loss:1.08279\n",
            "[101,    10] running loss:12.09512,  box loss: 0.00946, class loss:1.30554\n",
            "[101,    20] running loss:12.14762,  box loss: 0.00994, class loss:1.20701\n",
            "[102,    10] running loss:11.66625,  box loss: 0.00565, class loss:1.15786\n",
            "[102,    20] running loss:11.48137,  box loss: 0.00783, class loss:1.13354\n",
            "[103,    10] running loss:11.59215,  box loss: 0.00694, class loss:1.21037\n",
            "[103,    20] running loss:12.04256,  box loss: 0.00445, class loss:1.25580\n",
            "[104,    10] running loss:11.34015,  box loss: 0.00618, class loss:1.16844\n",
            "[104,    20] running loss:11.42744,  box loss: 0.00903, class loss:1.08278\n",
            "[105,    10] running loss:11.13467,  box loss: 0.01005, class loss:1.10852\n",
            "[105,    20] running loss:10.92058,  box loss: 0.00750, class loss:1.10642\n",
            "[106,    10] running loss:10.91531,  box loss: 0.00991, class loss:1.10629\n",
            "[106,    20] running loss:11.03724,  box loss: 0.01120, class loss:1.07520\n",
            "[107,    10] running loss:10.99090,  box loss: 0.00860, class loss:1.04418\n",
            "[107,    20] running loss:11.13260,  box loss: 0.01003, class loss:1.10588\n",
            "[108,    10] running loss:11.08247,  box loss: 0.00748, class loss:1.08624\n",
            "[108,    20] running loss:10.83543,  box loss: 0.00788, class loss:1.10309\n",
            "[109,    10] running loss:10.76380,  box loss: 0.00850, class loss:1.07482\n",
            "[109,    20] running loss:10.88974,  box loss: 0.00678, class loss:1.07485\n",
            "[110,    10] running loss:10.93719,  box loss: 0.00663, class loss:1.13702\n",
            "[110,    20] running loss:10.88999,  box loss: 0.00357, class loss:1.07970\n",
            "[111,    10] running loss:10.97571,  box loss: 0.00951, class loss:1.10607\n",
            "[111,    20] running loss:10.81302,  box loss: 0.00309, class loss:1.09218\n",
            "[112,    10] running loss:10.90032,  box loss: 0.00732, class loss:1.10708\n",
            "[112,    20] running loss:10.96349,  box loss: 0.00803, class loss:1.12185\n",
            "[113,    10] running loss:11.22451,  box loss: 0.00622, class loss:1.17552\n",
            "[113,    20] running loss:11.13789,  box loss: 0.00994, class loss:1.10257\n",
            "[114,    10] running loss:10.93369,  box loss: 0.00628, class loss:1.07500\n",
            "[114,    20] running loss:11.15759,  box loss: 0.00587, class loss:1.09938\n",
            "[115,    10] running loss:11.09544,  box loss: 0.00673, class loss:1.06705\n",
            "[115,    20] running loss:10.84228,  box loss: 0.00448, class loss:1.06170\n",
            "[116,    10] running loss:11.08910,  box loss: 0.00594, class loss:1.13750\n",
            "[116,    20] running loss:10.91284,  box loss: 0.00653, class loss:1.04529\n",
            "[117,    10] running loss:10.83522,  box loss: 0.00705, class loss:1.10651\n",
            "[117,    20] running loss:11.02950,  box loss: 0.00499, class loss:1.05367\n",
            "[118,    10] running loss:11.11719,  box loss: 0.00999, class loss:1.16099\n",
            "[118,    20] running loss:10.96184,  box loss: 0.00770, class loss:1.04364\n",
            "[119,    10] running loss:10.91064,  box loss: 0.00835, class loss:1.04368\n",
            "[119,    20] running loss:10.74595,  box loss: 0.00661, class loss:1.07363\n",
            "[120,    10] running loss:11.00206,  box loss: 0.00958, class loss:1.13319\n",
            "[120,    20] running loss:10.76201,  box loss: 0.00786, class loss:1.04371\n",
            "[121,    10] running loss:10.84170,  box loss: 0.00380, class loss:1.13735\n",
            "[121,    20] running loss:10.81957,  box loss: 0.00366, class loss:1.10610\n",
            "[122,    10] running loss:10.91339,  box loss: 0.00692, class loss:1.10612\n",
            "[122,    20] running loss:10.85916,  box loss: 0.00575, class loss:1.07767\n",
            "[123,    10] running loss:10.79181,  box loss: 0.00643, class loss:1.07280\n",
            "[123,    20] running loss:10.83285,  box loss: 0.00978, class loss:1.04371\n",
            "[124,    10] running loss:10.87505,  box loss: 0.00438, class loss:1.07485\n",
            "[124,    20] running loss:10.88165,  box loss: 0.00898, class loss:1.04398\n",
            "[125,    10] running loss:10.95236,  box loss: 0.00666, class loss:1.07403\n",
            "[125,    20] running loss:10.86387,  box loss: 0.00576, class loss:1.07507\n",
            "[126,    10] running loss:10.83872,  box loss: 0.00913, class loss:1.04397\n",
            "[126,    20] running loss:10.85680,  box loss: 0.00359, class loss:1.07512\n",
            "[127,    10] running loss:11.06252,  box loss: 0.00416, class loss:1.07488\n",
            "[127,    20] running loss:10.69283,  box loss: 0.00468, class loss:1.07486\n",
            "[128,    10] running loss:10.85635,  box loss: 0.00650, class loss:1.04364\n",
            "[128,    20] running loss:10.70001,  box loss: 0.00600, class loss:1.07485\n",
            "[129,    10] running loss:10.90289,  box loss: 0.00443, class loss:1.07482\n",
            "[129,    20] running loss:10.77811,  box loss: 0.00874, class loss:1.07353\n",
            "[130,    10] running loss:10.96629,  box loss: 0.00745, class loss:1.10667\n",
            "[130,    20] running loss:10.67110,  box loss: 0.00645, class loss:1.05193\n",
            "[131,    10] running loss:10.94310,  box loss: 0.00600, class loss:1.10629\n",
            "[131,    20] running loss:10.93358,  box loss: 0.00840, class loss:1.04370\n",
            "[132,    10] running loss:10.83764,  box loss: 0.00469, class loss:1.07641\n",
            "[132,    20] running loss:10.77375,  box loss: 0.00590, class loss:1.07755\n",
            "[133,    10] running loss:10.88535,  box loss: 0.00652, class loss:1.13526\n",
            "[133,    20] running loss:10.81637,  box loss: 0.01429, class loss:1.05004\n",
            "[134,    10] running loss:10.76289,  box loss: 0.00439, class loss:1.04359\n",
            "[134,    20] running loss:10.88734,  box loss: 0.00603, class loss:1.07349\n",
            "[135,    10] running loss:10.86947,  box loss: 0.00367, class loss:1.10591\n",
            "[135,    20] running loss:10.90563,  box loss: 0.00485, class loss:1.09671\n",
            "[136,    10] running loss:10.80947,  box loss: 0.00874, class loss:1.10405\n",
            "[136,    20] running loss:10.80481,  box loss: 0.00728, class loss:1.10312\n",
            "[137,    10] running loss:10.81092,  box loss: 0.00598, class loss:1.04360\n",
            "[137,    20] running loss:10.68500,  box loss: 0.00944, class loss:1.07493\n",
            "[138,    10] running loss:10.84987,  box loss: 0.01256, class loss:1.04379\n",
            "[138,    20] running loss:10.78881,  box loss: 0.00953, class loss:1.04360\n",
            "[139,    10] running loss:10.87188,  box loss: 0.01091, class loss:1.16824\n",
            "[139,    20] running loss:10.92050,  box loss: 0.01357, class loss:1.07278\n",
            "[140,    10] running loss:10.92554,  box loss: 0.00872, class loss:1.10609\n",
            "[140,    20] running loss:10.79905,  box loss: 0.01100, class loss:1.04367\n",
            "[141,    10] running loss:10.77392,  box loss: 0.00873, class loss:1.04653\n",
            "[141,    20] running loss:10.82044,  box loss: 0.00806, class loss:1.10599\n",
            "[142,    10] running loss:10.92507,  box loss: 0.00862, class loss:1.04367\n",
            "[142,    20] running loss:10.74026,  box loss: 0.00851, class loss:1.07393\n",
            "[143,    10] running loss:10.81703,  box loss: 0.01156, class loss:1.10389\n",
            "[143,    20] running loss:10.88753,  box loss: 0.00859, class loss:1.07489\n",
            "[144,    10] running loss:10.93022,  box loss: 0.00436, class loss:1.07502\n",
            "[144,    20] running loss:10.96112,  box loss: 0.00752, class loss:1.07681\n",
            "[145,    10] running loss:10.67735,  box loss: 0.00781, class loss:1.08566\n",
            "[145,    20] running loss:11.10408,  box loss: 0.00922, class loss:1.13725\n",
            "[146,    10] running loss:10.95864,  box loss: 0.00531, class loss:1.12830\n",
            "[146,    20] running loss:10.98894,  box loss: 0.00696, class loss:1.07621\n",
            "[147,    10] running loss:10.72163,  box loss: 0.01007, class loss:1.07641\n",
            "[147,    20] running loss:10.86687,  box loss: 0.00745, class loss:1.07477\n",
            "[148,    10] running loss:10.79599,  box loss: 0.01000, class loss:1.04361\n",
            "[148,    20] running loss:10.89460,  box loss: 0.00754, class loss:1.12999\n",
            "[149,    10] running loss:10.81133,  box loss: 0.01026, class loss:1.08088\n",
            "[149,    20] running loss:10.90913,  box loss: 0.00689, class loss:1.13647\n",
            "[150,    10] running loss:11.28913,  box loss: 0.01259, class loss:1.11105\n",
            "[150,    20] running loss:11.42188,  box loss: 0.00453, class loss:1.10809\n",
            "[151,    10] running loss:11.28771,  box loss: 0.00789, class loss:1.08272\n",
            "[151,    20] running loss:11.03588,  box loss: 0.00962, class loss:1.13524\n",
            "[152,    10] running loss:11.40158,  box loss: 0.00992, class loss:1.10835\n",
            "[152,    20] running loss:11.54757,  box loss: 0.00555, class loss:1.11805\n",
            "[153,    10] running loss:11.06101,  box loss: 0.00848, class loss:1.07277\n",
            "[153,    20] running loss:11.38423,  box loss: 0.01003, class loss:1.08591\n",
            "[154,    10] running loss:11.28681,  box loss: 0.00406, class loss:1.19476\n",
            "[154,    20] running loss:11.07648,  box loss: 0.00845, class loss:1.17082\n",
            "[155,    10] running loss:11.04936,  box loss: 0.00308, class loss:1.08538\n",
            "[155,    20] running loss:10.95386,  box loss: 0.00351, class loss:1.16586\n",
            "[156,    10] running loss:11.01008,  box loss: 0.00876, class loss:1.06885\n",
            "[156,    20] running loss:10.98698,  box loss: 0.00935, class loss:1.07501\n",
            "[157,    10] running loss:11.04113,  box loss: 0.00806, class loss:1.07483\n",
            "[157,    20] running loss:10.75893,  box loss: 0.00361, class loss:1.07561\n",
            "[158,    10] running loss:10.86272,  box loss: 0.00484, class loss:1.05234\n",
            "[158,    20] running loss:10.80651,  box loss: 0.00747, class loss:1.04360\n",
            "[159,    10] running loss:10.86588,  box loss: 0.00430, class loss:1.13727\n",
            "[159,    20] running loss:10.85994,  box loss: 0.00727, class loss:1.04637\n",
            "[160,    10] running loss:10.81375,  box loss: 0.00705, class loss:1.10729\n",
            "[160,    20] running loss:10.88437,  box loss: 0.00698, class loss:1.07487\n",
            "[161,    10] running loss:10.86525,  box loss: 0.00495, class loss:1.04361\n",
            "[161,    20] running loss:10.88845,  box loss: 0.01157, class loss:1.07498\n",
            "[162,    10] running loss:10.81007,  box loss: 0.00285, class loss:1.07600\n",
            "[162,    20] running loss:10.97050,  box loss: 0.00593, class loss:1.07483\n",
            "[163,    10] running loss:10.73664,  box loss: 0.01014, class loss:1.07813\n",
            "[163,    20] running loss:10.83335,  box loss: 0.00754, class loss:1.04362\n",
            "[164,    10] running loss:10.77231,  box loss: 0.00651, class loss:1.04359\n",
            "[164,    20] running loss:10.74214,  box loss: 0.00995, class loss:1.07481\n",
            "[165,    10] running loss:10.80313,  box loss: 0.00408, class loss:1.04362\n",
            "[165,    20] running loss:10.87117,  box loss: 0.00803, class loss:1.08106\n",
            "[166,    10] running loss:10.72240,  box loss: 0.00878, class loss:1.07485\n",
            "[166,    20] running loss:10.78354,  box loss: 0.00253, class loss:1.10609\n",
            "[167,    10] running loss:10.81250,  box loss: 0.00360, class loss:1.10670\n",
            "[167,    20] running loss:10.80155,  box loss: 0.00451, class loss:1.07484\n",
            "[168,    10] running loss:10.81861,  box loss: 0.00723, class loss:1.08227\n",
            "[168,    20] running loss:10.80240,  box loss: 0.00816, class loss:1.04392\n",
            "[169,    10] running loss:10.77978,  box loss: 0.00376, class loss:1.05154\n",
            "[169,    20] running loss:10.75856,  box loss: 0.00529, class loss:1.07461\n",
            "[170,    10] running loss:10.71669,  box loss: 0.00349, class loss:1.10609\n",
            "[170,    20] running loss:10.87917,  box loss: 0.00654, class loss:1.16690\n",
            "[171,    10] running loss:10.83395,  box loss: 0.00620, class loss:1.07487\n",
            "[171,    20] running loss:10.84855,  box loss: 0.00427, class loss:1.13697\n",
            "[172,    10] running loss:10.75966,  box loss: 0.00747, class loss:1.04367\n",
            "[172,    20] running loss:10.84705,  box loss: 0.00576, class loss:1.10997\n",
            "[173,    10] running loss:10.68835,  box loss: 0.00558, class loss:1.10633\n",
            "[173,    20] running loss:10.85573,  box loss: 0.00960, class loss:1.04364\n",
            "[174,    10] running loss:10.74092,  box loss: 0.00658, class loss:1.04372\n",
            "[174,    20] running loss:10.74684,  box loss: 0.00227, class loss:1.07383\n",
            "[175,    10] running loss:10.82276,  box loss: 0.00872, class loss:1.04360\n",
            "[175,    20] running loss:10.69849,  box loss: 0.00434, class loss:1.07484\n",
            "[176,    10] running loss:10.75291,  box loss: 0.00416, class loss:1.07483\n",
            "[176,    20] running loss:10.86875,  box loss: 0.00701, class loss:1.13734\n",
            "[177,    10] running loss:10.79640,  box loss: 0.00517, class loss:1.04359\n",
            "[177,    20] running loss:10.68890,  box loss: 0.00485, class loss:1.07427\n",
            "[178,    10] running loss:10.79261,  box loss: 0.00360, class loss:1.04359\n",
            "[178,    20] running loss:10.73064,  box loss: 0.00559, class loss:1.10658\n",
            "[179,    10] running loss:10.75024,  box loss: 0.00392, class loss:1.07489\n",
            "[179,    20] running loss:10.78261,  box loss: 0.00314, class loss:1.07484\n",
            "[180,    10] running loss:10.91670,  box loss: 0.00531, class loss:1.10609\n",
            "[180,    20] running loss:10.66606,  box loss: 0.01151, class loss:1.04359\n",
            "[181,    10] running loss:10.87210,  box loss: 0.00352, class loss:1.13738\n",
            "[181,    20] running loss:10.74180,  box loss: 0.00645, class loss:1.07484\n",
            "[182,    10] running loss:10.80767,  box loss: 0.00869, class loss:1.04360\n",
            "[182,    20] running loss:10.73603,  box loss: 0.00427, class loss:1.11770\n",
            "[183,    10] running loss:10.82112,  box loss: 0.00632, class loss:1.07813\n",
            "[183,    20] running loss:10.92186,  box loss: 0.00667, class loss:1.10690\n",
            "[184,    10] running loss:10.80168,  box loss: 0.00369, class loss:1.10653\n",
            "[184,    20] running loss:10.85692,  box loss: 0.00614, class loss:1.07496\n",
            "[185,    10] running loss:10.96890,  box loss: 0.00832, class loss:1.11830\n",
            "[185,    20] running loss:11.26592,  box loss: 0.01108, class loss:1.10317\n",
            "[186,    10] running loss:10.85234,  box loss: 0.00703, class loss:1.04953\n",
            "[186,    20] running loss:11.38403,  box loss: 0.00672, class loss:1.15963\n",
            "[187,    10] running loss:11.09180,  box loss: 0.00497, class loss:1.15159\n",
            "[187,    20] running loss:11.06374,  box loss: 0.00631, class loss:1.08128\n",
            "[188,    10] running loss:10.88817,  box loss: 0.01123, class loss:1.04392\n",
            "[188,    20] running loss:10.87629,  box loss: 0.00380, class loss:1.04362\n",
            "[189,    10] running loss:10.95473,  box loss: 0.00273, class loss:1.12946\n",
            "[189,    20] running loss:10.88330,  box loss: 0.00308, class loss:1.06835\n",
            "[190,    10] running loss:10.93661,  box loss: 0.00752, class loss:1.11033\n",
            "[190,    20] running loss:10.66980,  box loss: 0.00556, class loss:1.04366\n",
            "[191,    10] running loss:10.98510,  box loss: 0.00604, class loss:1.10621\n",
            "[191,    20] running loss:10.72064,  box loss: 0.01130, class loss:1.07410\n",
            "[192,    10] running loss:10.89889,  box loss: 0.00981, class loss:1.04360\n",
            "[192,    20] running loss:10.72042,  box loss: 0.01086, class loss:1.04361\n",
            "[193,    10] running loss:10.75661,  box loss: 0.01104, class loss:1.04360\n",
            "[193,    20] running loss:10.93582,  box loss: 0.00565, class loss:1.07055\n",
            "[194,    10] running loss:10.81834,  box loss: 0.00735, class loss:1.12525\n",
            "[194,    20] running loss:10.78274,  box loss: 0.00820, class loss:1.04360\n",
            "[195,    10] running loss:10.83998,  box loss: 0.00577, class loss:1.06945\n",
            "[195,    20] running loss:10.74667,  box loss: 0.01134, class loss:1.05116\n",
            "[196,    10] running loss:10.73542,  box loss: 0.00541, class loss:1.07511\n",
            "[196,    20] running loss:10.82110,  box loss: 0.00560, class loss:1.04384\n",
            "[197,    10] running loss:10.86231,  box loss: 0.00792, class loss:1.04462\n",
            "[197,    20] running loss:10.80465,  box loss: 0.01149, class loss:1.07972\n",
            "[198,    10] running loss:10.77833,  box loss: 0.00468, class loss:1.10608\n",
            "[198,    20] running loss:10.76720,  box loss: 0.00187, class loss:1.10519\n",
            "[199,    10] running loss:10.83963,  box loss: 0.00518, class loss:1.08422\n",
            "[199,    20] running loss:11.05780,  box loss: 0.00672, class loss:1.10309\n",
            "[200,    10] running loss:10.84334,  box loss: 0.00910, class loss:1.04359\n",
            "[200,    20] running loss:10.96269,  box loss: 0.00529, class loss:1.16858\n",
            "[201,    10] running loss:11.00012,  box loss: 0.00808, class loss:1.12374\n",
            "[201,    20] running loss:10.76035,  box loss: 0.00880, class loss:1.07497\n",
            "[202,    10] running loss:10.89775,  box loss: 0.00517, class loss:1.04364\n",
            "[202,    20] running loss:10.76871,  box loss: 0.00510, class loss:1.04359\n",
            "[203,    10] running loss:10.87084,  box loss: 0.00428, class loss:1.04364\n",
            "[203,    20] running loss:10.84299,  box loss: 0.00533, class loss:1.04360\n",
            "[204,    10] running loss:10.86901,  box loss: 0.00784, class loss:1.04896\n",
            "[204,    20] running loss:10.70162,  box loss: 0.00919, class loss:1.04359\n",
            "[205,    10] running loss:10.67337,  box loss: 0.00490, class loss:1.07485\n",
            "[205,    20] running loss:10.84832,  box loss: 0.00314, class loss:1.07478\n",
            "[206,    10] running loss:10.78512,  box loss: 0.00739, class loss:1.10406\n",
            "[206,    20] running loss:10.85120,  box loss: 0.00470, class loss:1.04361\n",
            "[207,    10] running loss:10.74836,  box loss: 0.00458, class loss:1.07499\n",
            "[207,    20] running loss:10.71707,  box loss: 0.01136, class loss:1.10610\n",
            "[208,    10] running loss:10.77320,  box loss: 0.00669, class loss:1.04359\n",
            "[208,    20] running loss:10.76533,  box loss: 0.00531, class loss:1.04585\n",
            "[209,    10] running loss:10.81801,  box loss: 0.01135, class loss:1.11075\n",
            "[209,    20] running loss:10.76246,  box loss: 0.00439, class loss:1.07481\n",
            "[210,    10] running loss:10.81439,  box loss: 0.00357, class loss:1.07486\n",
            "[210,    20] running loss:10.82949,  box loss: 0.00458, class loss:1.07340\n",
            "[211,    10] running loss:10.74510,  box loss: 0.00494, class loss:1.07313\n",
            "[211,    20] running loss:10.70703,  box loss: 0.00389, class loss:1.10493\n",
            "[212,    10] running loss:10.75057,  box loss: 0.00416, class loss:1.04359\n",
            "[212,    20] running loss:10.71984,  box loss: 0.00849, class loss:1.04359\n",
            "[213,    10] running loss:10.82287,  box loss: 0.00698, class loss:1.04359\n",
            "[213,    20] running loss:10.75293,  box loss: 0.00461, class loss:1.07484\n",
            "[214,    10] running loss:10.74899,  box loss: 0.00685, class loss:1.13732\n",
            "[214,    20] running loss:10.76572,  box loss: 0.00733, class loss:1.07379\n",
            "[215,    10] running loss:10.76471,  box loss: 0.00432, class loss:1.07484\n",
            "[215,    20] running loss:10.87356,  box loss: 0.00951, class loss:1.07626\n",
            "[216,    10] running loss:10.77552,  box loss: 0.00552, class loss:1.07424\n",
            "[216,    20] running loss:10.85346,  box loss: 0.00662, class loss:1.04372\n",
            "[217,    10] running loss:10.96284,  box loss: 0.00499, class loss:1.04359\n",
            "[217,    20] running loss:10.78448,  box loss: 0.00511, class loss:1.10273\n",
            "[218,    10] running loss:10.78969,  box loss: 0.00700, class loss:1.04359\n",
            "[218,    20] running loss:10.76214,  box loss: 0.00708, class loss:1.04363\n",
            "[219,    10] running loss:10.87979,  box loss: 0.00356, class loss:1.07484\n",
            "[219,    20] running loss:10.82256,  box loss: 0.00434, class loss:1.10648\n",
            "[220,    10] running loss:10.67169,  box loss: 0.00851, class loss:1.07484\n",
            "[220,    20] running loss:10.81426,  box loss: 0.00640, class loss:1.07336\n",
            "[221,    10] running loss:10.79298,  box loss: 0.00769, class loss:1.10412\n",
            "[221,    20] running loss:10.76747,  box loss: 0.00668, class loss:1.04360\n",
            "[222,    10] running loss:10.76452,  box loss: 0.00736, class loss:1.04359\n",
            "[222,    20] running loss:10.69045,  box loss: 0.00822, class loss:1.04359\n",
            "[223,    10] running loss:10.83557,  box loss: 0.00171, class loss:1.04365\n",
            "[223,    20] running loss:10.75966,  box loss: 0.00848, class loss:1.04360\n",
            "[224,    10] running loss:10.75339,  box loss: 0.01174, class loss:1.10599\n",
            "[224,    20] running loss:10.77442,  box loss: 0.00448, class loss:1.07483\n",
            "[225,    10] running loss:10.71060,  box loss: 0.00365, class loss:1.04368\n",
            "[225,    20] running loss:10.68288,  box loss: 0.00491, class loss:1.04359\n",
            "[226,    10] running loss:10.61329,  box loss: 0.00578, class loss:1.04359\n",
            "[226,    20] running loss:10.92011,  box loss: 0.00805, class loss:1.07498\n",
            "[227,    10] running loss:10.72838,  box loss: 0.00986, class loss:1.13132\n",
            "[227,    20] running loss:10.77308,  box loss: 0.00427, class loss:1.04360\n",
            "[228,    10] running loss:10.75768,  box loss: 0.00860, class loss:1.07490\n",
            "[228,    20] running loss:10.76264,  box loss: 0.00702, class loss:1.07483\n",
            "[229,    10] running loss:10.83727,  box loss: 0.00649, class loss:1.10594\n",
            "[229,    20] running loss:10.80766,  box loss: 0.00674, class loss:1.07484\n",
            "[230,    10] running loss:10.84413,  box loss: 0.00435, class loss:1.07533\n",
            "[230,    20] running loss:10.70234,  box loss: 0.00785, class loss:1.04360\n",
            "[231,    10] running loss:10.85381,  box loss: 0.00717, class loss:1.04359\n",
            "[231,    20] running loss:10.65756,  box loss: 0.00610, class loss:1.04359\n",
            "[232,    10] running loss:10.70802,  box loss: 0.00972, class loss:1.10607\n",
            "[232,    20] running loss:10.68693,  box loss: 0.00547, class loss:1.07499\n",
            "[233,    10] running loss:10.80647,  box loss: 0.00484, class loss:1.04361\n",
            "[233,    20] running loss:10.69402,  box loss: 0.00800, class loss:1.04514\n",
            "[234,    10] running loss:10.73934,  box loss: 0.01001, class loss:1.07484\n",
            "[234,    20] running loss:10.77006,  box loss: 0.00533, class loss:1.10539\n",
            "[235,    10] running loss:10.89419,  box loss: 0.00509, class loss:1.13734\n",
            "[235,    20] running loss:10.59804,  box loss: 0.00925, class loss:1.07555\n",
            "[236,    10] running loss:10.68657,  box loss: 0.00811, class loss:1.04360\n",
            "[236,    20] running loss:10.72617,  box loss: 0.00731, class loss:1.10546\n",
            "[237,    10] running loss:10.74594,  box loss: 0.00589, class loss:1.07482\n",
            "[237,    20] running loss:10.77629,  box loss: 0.00419, class loss:1.07484\n",
            "[238,    10] running loss:10.75285,  box loss: 0.00574, class loss:1.04465\n",
            "[238,    20] running loss:10.70264,  box loss: 0.00658, class loss:1.07484\n",
            "[239,    10] running loss:10.79795,  box loss: 0.00812, class loss:1.07498\n",
            "[239,    20] running loss:10.77940,  box loss: 0.00649, class loss:1.07484\n",
            "[240,    10] running loss:10.87380,  box loss: 0.00634, class loss:1.11773\n",
            "[240,    20] running loss:10.60217,  box loss: 0.00453, class loss:1.04359\n",
            "[241,    10] running loss:10.81298,  box loss: 0.00872, class loss:1.10430\n",
            "[241,    20] running loss:10.59190,  box loss: 0.00437, class loss:1.04370\n",
            "[242,    10] running loss:10.70352,  box loss: 0.00542, class loss:1.04404\n",
            "[242,    20] running loss:10.74993,  box loss: 0.00135, class loss:1.07481\n",
            "[243,    10] running loss:10.74149,  box loss: 0.00725, class loss:1.10635\n",
            "[243,    20] running loss:10.79453,  box loss: 0.00822, class loss:1.05927\n",
            "[244,    10] running loss:10.74088,  box loss: 0.00802, class loss:1.04568\n",
            "[244,    20] running loss:10.85951,  box loss: 0.00853, class loss:1.07484\n",
            "[245,    10] running loss:10.54154,  box loss: 0.00703, class loss:1.04359\n",
            "[245,    20] running loss:10.82855,  box loss: 0.00497, class loss:1.04482\n",
            "[246,    10] running loss:10.75585,  box loss: 0.00281, class loss:1.07479\n",
            "[246,    20] running loss:10.62747,  box loss: 0.00516, class loss:1.07486\n",
            "[247,    10] running loss:10.74926,  box loss: 0.00812, class loss:1.04359\n",
            "[247,    20] running loss:10.55572,  box loss: 0.00110, class loss:1.07468\n",
            "[248,    10] running loss:10.68900,  box loss: 0.00328, class loss:1.07486\n",
            "[248,    20] running loss:10.81547,  box loss: 0.00724, class loss:1.07484\n",
            "[249,    10] running loss:10.64413,  box loss: 0.00549, class loss:1.05034\n",
            "[249,    20] running loss:10.75381,  box loss: 0.00465, class loss:1.04360\n",
            "[250,    10] running loss:10.67895,  box loss: 0.00440, class loss:1.07485\n",
            "[250,    20] running loss:10.84428,  box loss: 0.00752, class loss:1.07484\n",
            "[251,    10] running loss:10.77457,  box loss: 0.00796, class loss:1.04360\n",
            "[251,    20] running loss:10.66859,  box loss: 0.00517, class loss:1.07902\n",
            "[252,    10] running loss:10.81352,  box loss: 0.01460, class loss:1.07480\n",
            "[252,    20] running loss:10.65757,  box loss: 0.00375, class loss:1.04360\n",
            "[253,    10] running loss:10.90114,  box loss: 0.00492, class loss:1.10557\n",
            "[253,    20] running loss:10.69898,  box loss: 0.00581, class loss:1.07982\n",
            "[254,    10] running loss:10.71375,  box loss: 0.00612, class loss:1.04387\n",
            "[254,    20] running loss:10.80063,  box loss: 0.00463, class loss:1.07545\n",
            "[255,    10] running loss:10.76376,  box loss: 0.00529, class loss:1.07487\n",
            "[255,    20] running loss:10.77293,  box loss: 0.00436, class loss:1.07487\n",
            "[256,    10] running loss:10.68095,  box loss: 0.00645, class loss:1.04360\n",
            "[256,    20] running loss:10.75029,  box loss: 0.00279, class loss:1.10615\n",
            "[257,    10] running loss:10.80433,  box loss: 0.00928, class loss:1.07442\n",
            "[257,    20] running loss:10.75005,  box loss: 0.00400, class loss:1.11614\n",
            "[258,    10] running loss:10.66747,  box loss: 0.00648, class loss:1.04359\n",
            "[258,    20] running loss:10.72137,  box loss: 0.00630, class loss:1.04359\n",
            "[259,    10] running loss:10.68722,  box loss: 0.00539, class loss:1.04360\n",
            "[259,    20] running loss:10.66316,  box loss: 0.00560, class loss:1.10610\n",
            "[260,    10] running loss:10.75215,  box loss: 0.00763, class loss:1.04499\n",
            "[260,    20] running loss:10.71979,  box loss: 0.00427, class loss:1.04619\n",
            "[261,    10] running loss:10.66995,  box loss: 0.00274, class loss:1.07483\n",
            "[261,    20] running loss:10.74812,  box loss: 0.00295, class loss:1.04359\n",
            "[262,    10] running loss:10.71901,  box loss: 0.00546, class loss:1.04359\n",
            "[262,    20] running loss:10.74906,  box loss: 0.00705, class loss:1.04360\n",
            "[263,    10] running loss:10.83353,  box loss: 0.00517, class loss:1.05144\n",
            "[263,    20] running loss:10.70674,  box loss: 0.00670, class loss:1.07540\n",
            "[264,    10] running loss:10.71726,  box loss: 0.00498, class loss:1.04418\n",
            "[264,    20] running loss:10.70924,  box loss: 0.00574, class loss:1.04359\n",
            "[265,    10] running loss:10.73201,  box loss: 0.00524, class loss:1.08898\n",
            "[265,    20] running loss:10.69612,  box loss: 0.00392, class loss:1.04362\n",
            "[266,    10] running loss:10.68580,  box loss: 0.00313, class loss:1.04360\n",
            "[266,    20] running loss:10.69109,  box loss: 0.00355, class loss:1.04401\n",
            "[267,    10] running loss:10.71663,  box loss: 0.00777, class loss:1.06120\n",
            "[267,    20] running loss:10.68821,  box loss: 0.00592, class loss:1.04359\n",
            "[268,    10] running loss:10.80023,  box loss: 0.00413, class loss:1.04362\n",
            "[268,    20] running loss:10.61997,  box loss: 0.00458, class loss:1.04666\n",
            "[269,    10] running loss:10.67971,  box loss: 0.00303, class loss:1.04360\n",
            "[269,    20] running loss:10.76837,  box loss: 0.00270, class loss:1.07484\n",
            "[270,    10] running loss:10.70587,  box loss: 0.00465, class loss:1.04359\n",
            "[270,    20] running loss:10.75986,  box loss: 0.00503, class loss:1.10624\n",
            "[271,    10] running loss:10.75027,  box loss: 0.00413, class loss:1.13619\n",
            "[271,    20] running loss:10.70955,  box loss: 0.00374, class loss:1.07467\n",
            "[272,    10] running loss:10.75494,  box loss: 0.00447, class loss:1.04359\n",
            "[272,    20] running loss:10.55085,  box loss: 0.00491, class loss:1.04359\n",
            "[273,    10] running loss:10.77559,  box loss: 0.00769, class loss:1.04359\n",
            "[273,    20] running loss:10.64545,  box loss: 0.00350, class loss:1.04359\n",
            "[274,    10] running loss:10.65723,  box loss: 0.00691, class loss:1.04528\n",
            "[274,    20] running loss:10.74357,  box loss: 0.00638, class loss:1.04360\n",
            "[275,    10] running loss:10.67846,  box loss: 0.00461, class loss:1.07475\n",
            "[275,    20] running loss:10.73746,  box loss: 0.00688, class loss:1.04359\n",
            "[276,    10] running loss:10.69170,  box loss: 0.00493, class loss:1.08109\n",
            "[276,    20] running loss:10.65221,  box loss: 0.00810, class loss:1.04359\n",
            "[277,    10] running loss:10.72805,  box loss: 0.00125, class loss:1.13662\n",
            "[277,    20] running loss:10.68146,  box loss: 0.00502, class loss:1.07483\n",
            "[278,    10] running loss:10.62275,  box loss: 0.00493, class loss:1.07489\n",
            "[278,    20] running loss:10.76810,  box loss: 0.00342, class loss:1.07498\n",
            "[279,    10] running loss:10.71819,  box loss: 0.00550, class loss:1.04360\n",
            "[279,    20] running loss:10.73227,  box loss: 0.00686, class loss:1.10610\n",
            "[280,    10] running loss:10.88628,  box loss: 0.00338, class loss:1.07319\n",
            "[280,    20] running loss:10.74439,  box loss: 0.00812, class loss:1.07485\n",
            "[281,    10] running loss:10.65088,  box loss: 0.00440, class loss:1.04360\n",
            "[281,    20] running loss:10.77864,  box loss: 0.00642, class loss:1.07520\n",
            "[282,    10] running loss:10.72680,  box loss: 0.00397, class loss:1.04360\n",
            "[282,    20] running loss:10.68246,  box loss: 0.00722, class loss:1.05331\n",
            "[283,    10] running loss:10.65581,  box loss: 0.00360, class loss:1.07357\n",
            "[283,    20] running loss:10.76206,  box loss: 0.00711, class loss:1.16807\n",
            "[284,    10] running loss:10.74542,  box loss: 0.00942, class loss:1.04359\n",
            "[284,    20] running loss:10.72050,  box loss: 0.00562, class loss:1.07484\n",
            "[285,    10] running loss:10.73408,  box loss: 0.00565, class loss:1.07485\n",
            "[285,    20] running loss:10.62133,  box loss: 0.00840, class loss:1.07535\n",
            "[286,    10] running loss:10.70987,  box loss: 0.00296, class loss:1.04363\n",
            "[286,    20] running loss:10.64660,  box loss: 0.00506, class loss:1.04359\n",
            "[287,    10] running loss:10.61844,  box loss: 0.00528, class loss:1.13681\n",
            "[287,    20] running loss:10.73637,  box loss: 0.00711, class loss:1.07484\n",
            "[288,    10] running loss:10.71886,  box loss: 0.00516, class loss:1.04359\n",
            "[288,    20] running loss:10.78256,  box loss: 0.00695, class loss:1.07492\n",
            "[289,    10] running loss:10.66443,  box loss: 0.00746, class loss:1.07485\n",
            "[289,    20] running loss:10.78674,  box loss: 0.00753, class loss:1.07541\n",
            "[290,    10] running loss:10.79934,  box loss: 0.00706, class loss:1.08386\n",
            "[290,    20] running loss:10.78076,  box loss: 0.00511, class loss:1.06655\n",
            "[291,    10] running loss:10.69812,  box loss: 0.00552, class loss:1.07484\n",
            "[291,    20] running loss:10.67608,  box loss: 0.00694, class loss:1.04364\n",
            "[292,    10] running loss:10.70846,  box loss: 0.00812, class loss:1.07484\n",
            "[292,    20] running loss:10.82598,  box loss: 0.00566, class loss:1.07387\n",
            "[293,    10] running loss:10.80899,  box loss: 0.00661, class loss:1.07429\n",
            "[293,    20] running loss:10.63658,  box loss: 0.00654, class loss:1.04388\n",
            "[294,    10] running loss:10.84951,  box loss: 0.00230, class loss:1.10292\n",
            "[294,    20] running loss:10.83662,  box loss: 0.00574, class loss:1.10531\n",
            "[295,    10] running loss:10.74946,  box loss: 0.00437, class loss:1.09508\n",
            "[295,    20] running loss:10.69128,  box loss: 0.00675, class loss:1.05574\n",
            "[296,    10] running loss:10.67181,  box loss: 0.00301, class loss:1.04382\n",
            "[296,    20] running loss:10.78711,  box loss: 0.00703, class loss:1.10556\n",
            "[297,    10] running loss:10.80790,  box loss: 0.00638, class loss:1.04737\n",
            "[297,    20] running loss:10.67636,  box loss: 0.00318, class loss:1.07484\n",
            "[298,    10] running loss:10.64179,  box loss: 0.00405, class loss:1.04385\n",
            "[298,    20] running loss:10.74953,  box loss: 0.01102, class loss:1.04360\n",
            "[299,    10] running loss:10.79765,  box loss: 0.00455, class loss:1.10687\n",
            "[299,    20] running loss:10.70415,  box loss: 0.00419, class loss:1.07484\n",
            "[300,    10] running loss:10.68213,  box loss: 0.00790, class loss:1.04361\n",
            "[300,    20] running loss:10.80900,  box loss: 0.00556, class loss:1.07484\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "id": "UXNlsLo1EU35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b669159-e10d-4c1b-e1e8-0ea5c5733328"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CatModel(\n",
              "  (regressor): Sequential(\n",
              "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Dropout(p=0.3, inplace=False)\n",
              "    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (5): ReLU()\n",
              "    (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "    (7): Flatten(start_dim=1, end_dim=-1)\n",
              "    (8): Linear(in_features=2048, out_features=256, bias=True)\n",
              "    (9): ReLU()\n",
              "    (10): Linear(in_features=256, out_features=128, bias=True)\n",
              "    (11): ReLU()\n",
              "    (12): Linear(in_features=128, out_features=32, bias=True)\n",
              "    (13): ReLU()\n",
              "    (14): Linear(in_features=32, out_features=4, bias=True)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "    (4): Dropout(p=0.3, inplace=False)\n",
              "    (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU()\n",
              "    (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (8): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "    (9): Flatten(start_dim=1, end_dim=-1)\n",
              "    (10): Linear(in_features=2048, out_features=128, bias=True)\n",
              "    (11): ReLU()\n",
              "    (12): Linear(in_features=128, out_features=32, bias=True)\n",
              "    (13): ReLU()\n",
              "    (14): Linear(in_features=32, out_features=6, bias=True)\n",
              "    (15): Softmax(dim=None)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def IOU(boxA, boxB):\n",
        "\t# determine the (x, y)-coordinates of the intersection rectangle\n",
        "\txA = max(boxA[0], boxB[0])\n",
        "\tyA = max(boxA[1], boxB[1])\n",
        "\txB = min(boxA[2], boxB[2])\n",
        "\tyB = min(boxA[3], boxB[3])\n",
        "\t# compute the area of intersection rectangle\n",
        "\tinterArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
        "\t# compute the area of both the prediction and ground-truth\n",
        "\t# rectangles\n",
        "\tboxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
        "\tboxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
        "\t# compute the intersection over union by taking the intersection\n",
        "\t# area and dividing it by the sum of prediction + ground-truth\n",
        "\t# areas - the interesection area\n",
        "\tiou = interArea / float(boxAArea + boxBArea - interArea)\n",
        "\t# return the intersection over union value\n",
        "\treturn iou"
      ],
      "metadata": {
        "id": "RfvveY4H2HYJ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_box(y_test,width,height):\n",
        "   box_width, box_height = y_test[2]*width, y_test[3]*height\n",
        "   x0,y0 = y_test[0]*width, y_test[1]*height\n",
        "   x1, y1 = x0 + box_width, y0 + box_height\n",
        "   return [x0,y0,x1,y1]"
      ],
      "metadata": {
        "id": "yLiVOstB6Lqj"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Accruacy of prediciton boundaries by IOU metric**"
      ],
      "metadata": {
        "id": "UlBcGxWn9hb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bound_accuracy, label_accuracy = 0, 0\n",
        "size = 0\n",
        "with torch.no_grad():\n",
        "  for i, data in enumerate(test_dataloader, 0):\n",
        "    X_test,y_test = data[0].to(device), data[1].to(device)\n",
        "    out = model(X_test)\n",
        "    for batch in range(y_test.shape[0]):\n",
        "      width,height = y_test[batch,5], y_test[batch,6]\n",
        "      actual_box = get_box(y_test[batch,1:4+1],width,height)\n",
        "      bound_pred, label_pred = out[0][batch],out[1][batch]\n",
        "      bound_pred = bound_pred.cpu().numpy()\n",
        "      label_pred = label_pred.cpu().numpy().argmax()\n",
        "      pred_box = get_box(bound_pred,width,height)\n",
        "      bound_accuracy+=IOU(actual_box,pred_box)\n",
        "      label_accuracy+= 1 if label_pred == y_test[batch,0] else 0\n",
        "      size+=1\n",
        "bound_accuracy /= size\n",
        "label_accuracy /=size\n",
        "print('IOU: ',float(bound_accuracy))\n",
        "print('Class accuracy: ',label_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaJOD0OM3qbm",
        "outputId": "8c95c2ef-2101-46d4-c385-94071a1b818d"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IOU:  0.5015634894371033\n",
            "Class accuracy:  0.42214532871972316\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RUNING_LOSS = 10.77457\n",
        "CLASS_LOSS = 1.07488\n",
        "BOX_LOSS = 0.00540\n",
        "EPOCH = 300\n",
        "PATH = '/content/drive/MyDrive/Colab Notebooks/cat_breed_recognizer/cat_breed_classifier_v3.pt'\n",
        "\n",
        "torch.save({\n",
        "    'epoch':EPOCH,\n",
        "    'class_loss':CLASS_LOSS,\n",
        "    'box_loss':BOX_LOSS,\n",
        "    'loss':RUNING_LOSS,\n",
        "    'model_state_dict':model.state_dict(),\n",
        "    'optimizer_state_dict':optimizer.state_dict()\n",
        "},PATH)\n"
      ],
      "metadata": {
        "id": "YK4KIEUC3E1l"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save(PATH_TO_DATA+'X.npy',X)\n",
        "np.save(PATH_TO_DATA+'y.npy',y)"
      ],
      "metadata": {
        "id": "oB7Ly54Z5aoI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}