{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "15_pkL6I6k_Ykjl0Sy086x50M17A9P_BI",
      "authorship_tag": "ABX9TyNKFiDuAvqMbKu0zWm/pyKG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/outsider8946/cat_breed_recognizer/blob/main/model/train_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "IC2jjdXb17jk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_TO_DATA = '/content/drive/MyDrive/Colab Notebooks/cat_breed_recognizer/data/'\n",
        "cat_breeds = ['bengal', 'maine_coon', 'ragdoll', 'oriental_shorthair', 'british_shorthair','siamese']"
      ],
      "metadata": {
        "id": "kUDloEn5-mr1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "device =  torch.device(dev)\n",
        "dev"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "rhbmytoa9Re1",
        "outputId": "5e93fbcf-38f1-4a9a-ed66-f3b81103c9db"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = []\n",
        "y = []\n",
        "for breed in cat_breeds:\n",
        "  print(breed)\n",
        "  if os.path.exists(PATH_TO_DATA+breed+f'/{breed}.csv'):\n",
        "    df = pd.read_csv(PATH_TO_DATA+breed+f'/{breed}.csv',on_bad_lines='skip')\n",
        "    for i in range(len(df)):\n",
        "      img_name = df.loc[i,'image_name']\n",
        "      img_mat = cv2.imread(PATH_TO_DATA+breed+f'/{img_name}')\n",
        "      img_mat = cv2.resize(img_mat,(32,32))\n",
        "      img_mat= img_mat/255\n",
        "      X.append(img_mat)\n",
        "      y.append(df.loc[i, ['label_name','norm_bbox_x', 'norm_bbox_y', 'norm_bbox_width', 'norm_bbox_height']].to_numpy().tolist())\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "X.shape, y.shape"
      ],
      "metadata": {
        "id": "2AJr4jzOOJ3q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "008ec201-990f-42a5-e405-e21c38a47576"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bengal\n",
            "maine_coon\n",
            "ragdoll\n",
            "oriental_shorthair\n",
            "british_shorthair\n",
            "siamese\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1155, 32, 32, 3), (1155, 5))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.load(PATH_TO_DATA+'X.npy')\n",
        "y = np.load(PATH_TO_DATA+'y.npy')"
      ],
      "metadata": {
        "id": "8OncBKyNQl-f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "outputId": "d99e908f-3411-48f6-ab30-0a235b250b31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-2eb18e6b5ed6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH_TO_DATA\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'X.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH_TO_DATA\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'y.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/cat_breed_recognizer/data/X.npy'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(data=y[:,0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "sUK_4X8Og6Do",
        "outputId": "1ae3ca19-71ed-4454-d549-8aae7cbd37e3"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: ylabel='Count'>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y,shuffle=True,random_state=42)\n",
        "\n",
        "X_train = torch.tensor(X_train).float().to(device)\n",
        "y_train = torch.tensor(y_train).float().to(device)\n",
        "X_test = torch.tensor(X_test).float().to(device)\n",
        "y_test = torch.tensor(y_test).float().to(device)\n",
        "\n",
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAO-2e57dF7r",
        "outputId": "8c55d735-0ed2-4beb-b141-2c552f0e2927"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([866, 32, 32, 3]),\n",
              " torch.Size([866, 5]),\n",
              " torch.Size([289, 32, 32, 3]),\n",
              " torch.Size([289, 5]))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_check = y_train.cpu().numpy()\n",
        "type(y_check)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPa73KU5eRpp",
        "outputId": "c31f528d-c816-42ac-e59e-63bcb9d23b90"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(data=y_check[:,0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "DuQMNOi8eaC3",
        "outputId": "49973f16-9537-49ad-da52-3720ff784ab4"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: ylabel='Count'>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CatDataset(Dataset):\n",
        "  def __init__(self,X,y):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "\n",
        "  def __getitem__(self,i):\n",
        "    return self.X[i], self.y[i]\n",
        "\n",
        "train_dataset = CatDataset(X_train,y_train)\n",
        "test_dataset = CatDataset(X_test,y_test)"
      ],
      "metadata": {
        "id": "Fc_em8LUGgUn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "train_dataloader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset,batch_size=batch_size,shuffle=True)"
      ],
      "metadata": {
        "id": "0aq7q5pS7gHD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CatModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.regressor = nn.Sequential(\n",
        "        nn.Conv2d(3, 32, kernel_size=(3,3), stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=(2, 2)),\n",
        "        nn.Dropout(0.3),\n",
        "\n",
        "        nn.Conv2d(32,32,kernel_size=(3,3),stride=1,padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=(2, 2)),\n",
        "        nn.Flatten(),\n",
        "\n",
        "        nn.Linear(8*8*32, 8*32),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Linear(8*32, 4*32),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Linear(4*32, 32),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Linear(32, 4),\n",
        "     )\n",
        "\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Conv2d(3, 32, kernel_size=(3,3), stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.MaxPool2d(kernel_size=(2, 2)),\n",
        "        nn.Dropout(0.3),\n",
        "\n",
        "        nn.Conv2d(32,32,kernel_size=(3,3),stride=1,padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.MaxPool2d(kernel_size=(2, 2)),\n",
        "        nn.Flatten(),\n",
        "\n",
        "        nn.Linear(8*8*32, 4*32),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Linear(4*32, 32),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Linear(32, 6),\n",
        "        nn.Softmax()\n",
        "        #nn.BatchNorm1d(6)\n",
        "      )\n",
        "\n",
        "  def forward(self,x):\n",
        "    # reshape\n",
        "    x = x.view(x.shape[0], 3, 32,32)\n",
        "    return self.regressor(x), self.classifier(x)\n"
      ],
      "metadata": {
        "id": "NJAL6-Tp2AzE"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CatModel()\n",
        "model.to(device)\n",
        "bbox_loss = nn.MSELoss()\n",
        "label_loss = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "n_epochs = 300\n",
        "for epoch in range(n_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_dataloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        input, targets = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        #convert targets to correct dimension\n",
        "        labels = np.array([[0]*6]*targets.shape[0],dtype=np.float64)\n",
        "        for batch in range(targets.shape[0]):\n",
        "          labels[batch,int(targets[batch,0].cpu())] = 1\n",
        "        labels = torch.tensor(labels).float().to(device)\n",
        "        bounds = targets[:,1:]\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        output = model(input)\n",
        "        regressor_loss = bbox_loss(output[0], bounds)\n",
        "\n",
        "        classifier_loss = label_loss(output[1],labels)\n",
        "        total_loss = classifier_loss + regressor_loss\n",
        "\n",
        "        #total_loss.backward()\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += total_loss.item()\n",
        "        if i % 10 == 9:    # print every 100 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] running loss:{running_loss:.5f},  box loss: {regressor_loss:.5f}, class loss:{classifier_loss:.5f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "id": "ShwYn06Z6_ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "900355f6-7a99-42aa-f3b7-495d33a0856a"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,    10] running loss:18.81569,  box loss: 0.04908, class loss:1.78117\n",
            "[1,    20] running loss:18.01116,  box loss: 0.03719, class loss:1.73219\n",
            "[2,    10] running loss:17.27910,  box loss: 0.04203, class loss:1.66375\n",
            "[2,    20] running loss:17.51834,  box loss: 0.04565, class loss:1.61413\n",
            "[3,    10] running loss:16.95367,  box loss: 0.04565, class loss:1.56584\n",
            "[3,    20] running loss:16.93849,  box loss: 0.04077, class loss:1.62678\n",
            "[4,    10] running loss:16.61574,  box loss: 0.03110, class loss:1.68136\n",
            "[4,    20] running loss:16.36059,  box loss: 0.03587, class loss:1.68433\n",
            "[5,    10] running loss:16.01281,  box loss: 0.03918, class loss:1.58088\n",
            "[5,    20] running loss:16.18775,  box loss: 0.03651, class loss:1.57232\n",
            "[6,    10] running loss:15.25941,  box loss: 0.03775, class loss:1.49519\n",
            "[6,    20] running loss:16.22167,  box loss: 0.03756, class loss:1.46711\n",
            "[7,    10] running loss:15.60664,  box loss: 0.06098, class loss:1.45477\n",
            "[7,    20] running loss:15.58304,  box loss: 0.03979, class loss:1.56883\n",
            "[8,    10] running loss:15.58447,  box loss: 0.03828, class loss:1.72245\n",
            "[8,    20] running loss:15.35686,  box loss: 0.02926, class loss:1.62636\n",
            "[9,    10] running loss:14.99613,  box loss: 0.04575, class loss:1.53157\n",
            "[9,    20] running loss:15.36478,  box loss: 0.02161, class loss:1.63706\n",
            "[10,    10] running loss:14.37949,  box loss: 0.03939, class loss:1.38956\n",
            "[10,    20] running loss:14.91267,  box loss: 0.03270, class loss:1.46150\n",
            "[11,    10] running loss:14.40690,  box loss: 0.02789, class loss:1.25793\n",
            "[11,    20] running loss:14.96069,  box loss: 0.04331, class loss:1.43297\n",
            "[12,    10] running loss:14.26954,  box loss: 0.03631, class loss:1.22793\n",
            "[12,    20] running loss:14.26069,  box loss: 0.03271, class loss:1.30104\n",
            "[13,    10] running loss:13.88421,  box loss: 0.03560, class loss:1.36653\n",
            "[13,    20] running loss:14.35732,  box loss: 0.03564, class loss:1.45798\n",
            "[14,    10] running loss:13.51869,  box loss: 0.02870, class loss:1.31658\n",
            "[14,    20] running loss:13.93262,  box loss: 0.03577, class loss:1.32139\n",
            "[15,    10] running loss:13.57648,  box loss: 0.02802, class loss:1.40142\n",
            "[15,    20] running loss:13.62202,  box loss: 0.02981, class loss:1.41343\n",
            "[16,    10] running loss:13.54915,  box loss: 0.03042, class loss:1.21228\n",
            "[16,    20] running loss:13.66924,  box loss: 0.02810, class loss:1.26243\n",
            "[17,    10] running loss:14.18037,  box loss: 0.03181, class loss:1.44203\n",
            "[17,    20] running loss:13.90407,  box loss: 0.03216, class loss:1.35115\n",
            "[18,    10] running loss:13.03318,  box loss: 0.02981, class loss:1.18202\n",
            "[18,    20] running loss:12.68714,  box loss: 0.02123, class loss:1.27721\n",
            "[19,    10] running loss:13.10398,  box loss: 0.01665, class loss:1.21366\n",
            "[19,    20] running loss:12.61247,  box loss: 0.02094, class loss:1.27609\n",
            "[20,    10] running loss:12.13857,  box loss: 0.02204, class loss:1.22768\n",
            "[20,    20] running loss:12.48601,  box loss: 0.02502, class loss:1.26592\n",
            "[21,    10] running loss:12.38378,  box loss: 0.02825, class loss:1.19643\n",
            "[21,    20] running loss:12.81982,  box loss: 0.02700, class loss:1.22633\n",
            "[22,    10] running loss:12.45162,  box loss: 0.02763, class loss:1.18277\n",
            "[22,    20] running loss:12.21103,  box loss: 0.01900, class loss:1.19637\n",
            "[23,    10] running loss:12.50757,  box loss: 0.02825, class loss:1.25978\n",
            "[23,    20] running loss:12.50349,  box loss: 0.02773, class loss:1.28927\n",
            "[24,    10] running loss:12.63972,  box loss: 0.02862, class loss:1.28712\n",
            "[24,    20] running loss:12.24936,  box loss: 0.02793, class loss:1.17787\n",
            "[25,    10] running loss:12.41208,  box loss: 0.02207, class loss:1.21671\n",
            "[25,    20] running loss:12.69195,  box loss: 0.02782, class loss:1.27267\n",
            "[26,    10] running loss:12.27958,  box loss: 0.02892, class loss:1.25808\n",
            "[26,    20] running loss:12.21584,  box loss: 0.02331, class loss:1.16728\n",
            "[27,    10] running loss:12.08442,  box loss: 0.02096, class loss:1.18467\n",
            "[27,    20] running loss:11.83352,  box loss: 0.02072, class loss:1.10730\n",
            "[28,    10] running loss:11.79603,  box loss: 0.02307, class loss:1.14452\n",
            "[28,    20] running loss:12.13187,  box loss: 0.01962, class loss:1.34304\n",
            "[29,    10] running loss:11.95940,  box loss: 0.01727, class loss:1.19014\n",
            "[29,    20] running loss:11.98350,  box loss: 0.02571, class loss:1.17809\n",
            "[30,    10] running loss:11.96498,  box loss: 0.02308, class loss:1.10395\n",
            "[30,    20] running loss:11.67413,  box loss: 0.01972, class loss:1.19883\n",
            "[31,    10] running loss:11.63911,  box loss: 0.02192, class loss:1.16845\n",
            "[31,    20] running loss:11.71227,  box loss: 0.02209, class loss:1.18048\n",
            "[32,    10] running loss:12.04359,  box loss: 0.02295, class loss:1.18900\n",
            "[32,    20] running loss:12.14999,  box loss: 0.02577, class loss:1.26420\n",
            "[33,    10] running loss:11.81245,  box loss: 0.02039, class loss:1.14757\n",
            "[33,    20] running loss:11.64930,  box loss: 0.02150, class loss:1.13563\n",
            "[34,    10] running loss:11.53370,  box loss: 0.01556, class loss:1.12537\n",
            "[34,    20] running loss:11.91505,  box loss: 0.01861, class loss:1.12517\n",
            "[35,    10] running loss:11.45402,  box loss: 0.02024, class loss:1.13919\n",
            "[35,    20] running loss:11.96212,  box loss: 0.02154, class loss:1.16351\n",
            "[36,    10] running loss:11.50600,  box loss: 0.01746, class loss:1.17464\n",
            "[36,    20] running loss:11.75085,  box loss: 0.01314, class loss:1.13777\n",
            "[37,    10] running loss:11.76342,  box loss: 0.01898, class loss:1.10512\n",
            "[37,    20] running loss:11.56270,  box loss: 0.01973, class loss:1.17754\n",
            "[38,    10] running loss:12.63234,  box loss: 0.01422, class loss:1.38098\n",
            "[38,    20] running loss:12.60395,  box loss: 0.02141, class loss:1.27043\n",
            "[39,    10] running loss:12.02202,  box loss: 0.02035, class loss:1.15763\n",
            "[39,    20] running loss:12.24844,  box loss: 0.01419, class loss:1.26788\n",
            "[40,    10] running loss:11.96253,  box loss: 0.02238, class loss:1.15880\n",
            "[40,    20] running loss:11.96686,  box loss: 0.01479, class loss:1.22986\n",
            "[41,    10] running loss:11.66402,  box loss: 0.01295, class loss:1.10883\n",
            "[41,    20] running loss:11.70172,  box loss: 0.01772, class loss:1.10650\n",
            "[42,    10] running loss:11.42344,  box loss: 0.01416, class loss:1.09184\n",
            "[42,    20] running loss:11.86971,  box loss: 0.01496, class loss:1.19271\n",
            "[43,    10] running loss:11.55284,  box loss: 0.01461, class loss:1.13739\n",
            "[43,    20] running loss:11.30358,  box loss: 0.01515, class loss:1.16818\n",
            "[44,    10] running loss:11.37781,  box loss: 0.01566, class loss:1.04413\n",
            "[44,    20] running loss:11.66750,  box loss: 0.01038, class loss:1.13618\n",
            "[45,    10] running loss:11.52649,  box loss: 0.01512, class loss:1.15412\n",
            "[45,    20] running loss:11.66138,  box loss: 0.01275, class loss:1.09466\n",
            "[46,    10] running loss:11.46834,  box loss: 0.01841, class loss:1.10859\n",
            "[46,    20] running loss:11.39966,  box loss: 0.01450, class loss:1.08606\n",
            "[47,    10] running loss:11.54967,  box loss: 0.01204, class loss:1.08218\n",
            "[47,    20] running loss:11.36981,  box loss: 0.01268, class loss:1.10617\n",
            "[48,    10] running loss:11.54277,  box loss: 0.01120, class loss:1.16616\n",
            "[48,    20] running loss:11.33269,  box loss: 0.01565, class loss:1.07668\n",
            "[49,    10] running loss:11.17888,  box loss: 0.01191, class loss:1.10545\n",
            "[49,    20] running loss:11.48087,  box loss: 0.01209, class loss:1.16693\n",
            "[50,    10] running loss:11.14145,  box loss: 0.01169, class loss:1.12143\n",
            "[50,    20] running loss:11.58184,  box loss: 0.00814, class loss:1.24350\n",
            "[51,    10] running loss:11.22687,  box loss: 0.01147, class loss:1.10616\n",
            "[51,    20] running loss:11.38047,  box loss: 0.01312, class loss:1.19668\n",
            "[52,    10] running loss:11.50761,  box loss: 0.01158, class loss:1.13826\n",
            "[52,    20] running loss:11.28712,  box loss: 0.00781, class loss:1.07341\n",
            "[53,    10] running loss:12.00502,  box loss: 0.01465, class loss:1.15221\n",
            "[53,    20] running loss:11.73181,  box loss: 0.01134, class loss:1.15228\n",
            "[54,    10] running loss:12.00520,  box loss: 0.01294, class loss:1.15621\n",
            "[54,    20] running loss:12.58721,  box loss: 0.00933, class loss:1.26433\n",
            "[55,    10] running loss:11.94200,  box loss: 0.01210, class loss:1.17951\n",
            "[55,    20] running loss:11.94723,  box loss: 0.01312, class loss:1.15811\n",
            "[56,    10] running loss:11.74130,  box loss: 0.00860, class loss:1.16441\n",
            "[56,    20] running loss:11.47713,  box loss: 0.01033, class loss:1.16861\n",
            "[57,    10] running loss:11.61796,  box loss: 0.00638, class loss:1.19743\n",
            "[57,    20] running loss:11.27326,  box loss: 0.01203, class loss:1.07621\n",
            "[58,    10] running loss:11.47860,  box loss: 0.00866, class loss:1.07809\n",
            "[58,    20] running loss:11.53438,  box loss: 0.01013, class loss:1.10788\n",
            "[59,    10] running loss:11.31633,  box loss: 0.00854, class loss:1.13839\n",
            "[59,    20] running loss:11.32381,  box loss: 0.01460, class loss:1.07455\n",
            "[60,    10] running loss:11.49655,  box loss: 0.00893, class loss:1.10714\n",
            "[60,    20] running loss:11.32382,  box loss: 0.01034, class loss:1.08562\n",
            "[61,    10] running loss:11.35956,  box loss: 0.01072, class loss:1.14376\n",
            "[61,    20] running loss:11.43300,  box loss: 0.00962, class loss:1.13560\n",
            "[62,    10] running loss:11.20931,  box loss: 0.00893, class loss:1.10401\n",
            "[62,    20] running loss:11.22018,  box loss: 0.01434, class loss:1.07513\n",
            "[63,    10] running loss:11.21514,  box loss: 0.01625, class loss:1.13633\n",
            "[63,    20] running loss:11.30617,  box loss: 0.00901, class loss:1.04759\n",
            "[64,    10] running loss:11.19826,  box loss: 0.00753, class loss:1.04377\n",
            "[64,    20] running loss:11.25194,  box loss: 0.01667, class loss:1.07573\n",
            "[65,    10] running loss:11.64266,  box loss: 0.01274, class loss:1.18806\n",
            "[65,    20] running loss:11.50083,  box loss: 0.01400, class loss:1.13811\n",
            "[66,    10] running loss:11.20307,  box loss: 0.00895, class loss:1.13482\n",
            "[66,    20] running loss:11.38407,  box loss: 0.00862, class loss:1.04560\n",
            "[67,    10] running loss:11.47505,  box loss: 0.00978, class loss:1.13208\n",
            "[67,    20] running loss:11.08822,  box loss: 0.00591, class loss:1.09968\n",
            "[68,    10] running loss:11.15208,  box loss: 0.01067, class loss:1.13122\n",
            "[68,    20] running loss:11.40906,  box loss: 0.00840, class loss:1.11348\n",
            "[69,    10] running loss:11.18203,  box loss: 0.00830, class loss:1.21479\n",
            "[69,    20] running loss:11.32848,  box loss: 0.00952, class loss:1.04378\n",
            "[70,    10] running loss:11.34988,  box loss: 0.01078, class loss:1.13736\n",
            "[70,    20] running loss:11.12837,  box loss: 0.01183, class loss:1.04537\n",
            "[71,    10] running loss:11.04588,  box loss: 0.01240, class loss:1.09123\n",
            "[71,    20] running loss:11.24580,  box loss: 0.00846, class loss:1.13585\n",
            "[72,    10] running loss:11.05796,  box loss: 0.00571, class loss:1.10606\n",
            "[72,    20] running loss:11.29620,  box loss: 0.01007, class loss:1.13998\n",
            "[73,    10] running loss:11.25015,  box loss: 0.00638, class loss:1.07503\n",
            "[73,    20] running loss:11.07982,  box loss: 0.01004, class loss:1.08117\n",
            "[74,    10] running loss:11.23485,  box loss: 0.01115, class loss:1.07544\n",
            "[74,    20] running loss:11.15946,  box loss: 0.01228, class loss:1.13387\n",
            "[75,    10] running loss:11.23465,  box loss: 0.00759, class loss:1.07338\n",
            "[75,    20] running loss:10.98804,  box loss: 0.00555, class loss:1.10533\n",
            "[76,    10] running loss:11.24304,  box loss: 0.01069, class loss:1.08561\n",
            "[76,    20] running loss:11.31591,  box loss: 0.01008, class loss:1.11841\n",
            "[77,    10] running loss:11.38981,  box loss: 0.01077, class loss:1.10605\n",
            "[77,    20] running loss:10.97015,  box loss: 0.01169, class loss:1.10498\n",
            "[78,    10] running loss:11.07694,  box loss: 0.00767, class loss:1.17249\n",
            "[78,    20] running loss:11.09574,  box loss: 0.00743, class loss:1.10620\n",
            "[79,    10] running loss:11.09673,  box loss: 0.00874, class loss:1.19225\n",
            "[79,    20] running loss:11.03125,  box loss: 0.00888, class loss:1.19845\n",
            "[80,    10] running loss:11.12978,  box loss: 0.00741, class loss:1.07310\n",
            "[80,    20] running loss:11.16278,  box loss: 0.00885, class loss:1.11513\n",
            "[81,    10] running loss:11.13185,  box loss: 0.00799, class loss:1.10431\n",
            "[81,    20] running loss:11.00697,  box loss: 0.00917, class loss:1.10438\n",
            "[82,    10] running loss:11.05671,  box loss: 0.00524, class loss:1.13555\n",
            "[82,    20] running loss:11.16421,  box loss: 0.00999, class loss:1.13544\n",
            "[83,    10] running loss:11.26970,  box loss: 0.00731, class loss:1.16847\n",
            "[83,    20] running loss:10.88569,  box loss: 0.00857, class loss:1.13928\n",
            "[84,    10] running loss:11.03911,  box loss: 0.00689, class loss:1.07497\n",
            "[84,    20] running loss:11.03853,  box loss: 0.01137, class loss:1.13661\n",
            "[85,    10] running loss:10.99216,  box loss: 0.00642, class loss:1.15048\n",
            "[85,    20] running loss:11.01366,  box loss: 0.00837, class loss:1.10896\n",
            "[86,    10] running loss:11.01437,  box loss: 0.00571, class loss:1.10869\n",
            "[86,    20] running loss:11.20023,  box loss: 0.00641, class loss:1.13747\n",
            "[87,    10] running loss:11.08691,  box loss: 0.00626, class loss:1.07697\n",
            "[87,    20] running loss:10.95506,  box loss: 0.00508, class loss:1.04369\n",
            "[88,    10] running loss:10.93564,  box loss: 0.01049, class loss:1.13848\n",
            "[88,    20] running loss:11.01128,  box loss: 0.00576, class loss:1.13819\n",
            "[89,    10] running loss:11.24439,  box loss: 0.00830, class loss:1.13732\n",
            "[89,    20] running loss:10.96399,  box loss: 0.00616, class loss:1.10535\n",
            "[90,    10] running loss:11.08716,  box loss: 0.00765, class loss:1.16648\n",
            "[90,    20] running loss:10.94832,  box loss: 0.00679, class loss:1.04364\n",
            "[91,    10] running loss:11.06838,  box loss: 0.00705, class loss:1.10897\n",
            "[91,    20] running loss:11.00753,  box loss: 0.01187, class loss:1.07511\n",
            "[92,    10] running loss:11.04120,  box loss: 0.00953, class loss:1.10638\n",
            "[92,    20] running loss:11.00140,  box loss: 0.00931, class loss:1.07837\n",
            "[93,    10] running loss:10.88809,  box loss: 0.01010, class loss:1.07278\n",
            "[93,    20] running loss:11.10478,  box loss: 0.00850, class loss:1.10004\n",
            "[94,    10] running loss:11.01585,  box loss: 0.01322, class loss:1.07486\n",
            "[94,    20] running loss:11.06158,  box loss: 0.00512, class loss:1.14626\n",
            "[95,    10] running loss:11.36519,  box loss: 0.00898, class loss:1.25333\n",
            "[95,    20] running loss:10.93342,  box loss: 0.00791, class loss:1.07757\n",
            "[96,    10] running loss:11.11636,  box loss: 0.01153, class loss:1.07370\n",
            "[96,    20] running loss:11.03084,  box loss: 0.00813, class loss:1.04484\n",
            "[97,    10] running loss:11.02791,  box loss: 0.00809, class loss:1.04407\n",
            "[97,    20] running loss:10.96326,  box loss: 0.00669, class loss:1.10687\n",
            "[98,    10] running loss:10.90481,  box loss: 0.00675, class loss:1.06939\n",
            "[98,    20] running loss:11.21750,  box loss: 0.00817, class loss:1.13852\n",
            "[99,    10] running loss:11.01837,  box loss: 0.00888, class loss:1.07514\n",
            "[99,    20] running loss:11.03992,  box loss: 0.00964, class loss:1.13592\n",
            "[100,    10] running loss:10.89385,  box loss: 0.00831, class loss:1.13197\n",
            "[100,    20] running loss:11.18866,  box loss: 0.01576, class loss:1.07426\n",
            "[101,    10] running loss:10.96608,  box loss: 0.00851, class loss:1.04477\n",
            "[101,    20] running loss:11.07365,  box loss: 0.00739, class loss:1.07598\n",
            "[102,    10] running loss:10.85519,  box loss: 0.00639, class loss:1.04467\n",
            "[102,    20] running loss:10.89733,  box loss: 0.00352, class loss:1.10649\n",
            "[103,    10] running loss:11.04580,  box loss: 0.00507, class loss:1.07516\n",
            "[103,    20] running loss:11.04610,  box loss: 0.01097, class loss:1.07341\n",
            "[104,    10] running loss:10.92279,  box loss: 0.00665, class loss:1.07573\n",
            "[104,    20] running loss:10.78941,  box loss: 0.00691, class loss:1.12724\n",
            "[105,    10] running loss:10.93219,  box loss: 0.00804, class loss:1.08148\n",
            "[105,    20] running loss:10.93742,  box loss: 0.00684, class loss:1.10805\n",
            "[106,    10] running loss:10.95484,  box loss: 0.00835, class loss:1.07497\n",
            "[106,    20] running loss:11.08876,  box loss: 0.01027, class loss:1.07402\n",
            "[107,    10] running loss:10.86685,  box loss: 0.00648, class loss:1.04383\n",
            "[107,    20] running loss:10.96864,  box loss: 0.01576, class loss:1.05442\n",
            "[108,    10] running loss:10.88283,  box loss: 0.01098, class loss:1.13421\n",
            "[108,    20] running loss:10.90638,  box loss: 0.00854, class loss:1.07497\n",
            "[109,    10] running loss:10.88171,  box loss: 0.00629, class loss:1.04370\n",
            "[109,    20] running loss:10.86361,  box loss: 0.00613, class loss:1.10624\n",
            "[110,    10] running loss:10.76325,  box loss: 0.00593, class loss:1.10532\n",
            "[110,    20] running loss:10.94804,  box loss: 0.00840, class loss:1.07493\n",
            "[111,    10] running loss:10.86010,  box loss: 0.00645, class loss:1.10311\n",
            "[111,    20] running loss:10.84893,  box loss: 0.00694, class loss:1.07446\n",
            "[112,    10] running loss:10.93478,  box loss: 0.00958, class loss:1.04365\n",
            "[112,    20] running loss:10.92185,  box loss: 0.00844, class loss:1.11052\n",
            "[113,    10] running loss:10.91778,  box loss: 0.00614, class loss:1.04359\n",
            "[113,    20] running loss:11.03221,  box loss: 0.00794, class loss:1.11755\n",
            "[114,    10] running loss:11.02167,  box loss: 0.00766, class loss:1.10630\n",
            "[114,    20] running loss:10.99475,  box loss: 0.00854, class loss:1.04901\n",
            "[115,    10] running loss:10.99567,  box loss: 0.01001, class loss:1.12116\n",
            "[115,    20] running loss:10.95033,  box loss: 0.00711, class loss:1.07490\n",
            "[116,    10] running loss:10.90265,  box loss: 0.01364, class loss:1.07448\n",
            "[116,    20] running loss:10.89154,  box loss: 0.00811, class loss:1.10719\n",
            "[117,    10] running loss:10.89304,  box loss: 0.00831, class loss:1.07481\n",
            "[117,    20] running loss:10.93561,  box loss: 0.00740, class loss:1.04555\n",
            "[118,    10] running loss:11.03827,  box loss: 0.00276, class loss:1.10641\n",
            "[118,    20] running loss:10.82846,  box loss: 0.00616, class loss:1.10648\n",
            "[119,    10] running loss:10.89751,  box loss: 0.00452, class loss:1.08026\n",
            "[119,    20] running loss:10.91896,  box loss: 0.00836, class loss:1.04360\n",
            "[120,    10] running loss:10.89078,  box loss: 0.00387, class loss:1.04438\n",
            "[120,    20] running loss:10.89333,  box loss: 0.00799, class loss:1.10365\n",
            "[121,    10] running loss:10.81035,  box loss: 0.00479, class loss:1.04369\n",
            "[121,    20] running loss:10.73978,  box loss: 0.00758, class loss:1.10624\n",
            "[122,    10] running loss:10.69272,  box loss: 0.00428, class loss:1.07492\n",
            "[122,    20] running loss:10.88632,  box loss: 0.00728, class loss:1.04361\n",
            "[123,    10] running loss:10.85145,  box loss: 0.00653, class loss:1.07631\n",
            "[123,    20] running loss:10.91037,  box loss: 0.00778, class loss:1.16553\n",
            "[124,    10] running loss:10.90603,  box loss: 0.00409, class loss:1.07454\n",
            "[124,    20] running loss:10.82466,  box loss: 0.01088, class loss:1.10609\n",
            "[125,    10] running loss:10.76760,  box loss: 0.00732, class loss:1.08076\n",
            "[125,    20] running loss:10.92365,  box loss: 0.01459, class loss:1.04418\n",
            "[126,    10] running loss:10.90337,  box loss: 0.00689, class loss:1.04361\n",
            "[126,    20] running loss:10.92090,  box loss: 0.00835, class loss:1.10614\n",
            "[127,    10] running loss:10.85531,  box loss: 0.00823, class loss:1.07460\n",
            "[127,    20] running loss:10.89187,  box loss: 0.00711, class loss:1.10623\n",
            "[128,    10] running loss:10.78182,  box loss: 0.00853, class loss:1.05957\n",
            "[128,    20] running loss:11.01101,  box loss: 0.00456, class loss:1.08901\n",
            "[129,    10] running loss:10.92642,  box loss: 0.00855, class loss:1.04522\n",
            "[129,    20] running loss:10.99999,  box loss: 0.00457, class loss:1.07640\n",
            "[130,    10] running loss:10.92638,  box loss: 0.00841, class loss:1.16740\n",
            "[130,    20] running loss:10.95853,  box loss: 0.00402, class loss:1.04360\n",
            "[131,    10] running loss:10.86246,  box loss: 0.00786, class loss:1.04882\n",
            "[131,    20] running loss:10.92940,  box loss: 0.00569, class loss:1.07521\n",
            "[132,    10] running loss:10.87875,  box loss: 0.00804, class loss:1.07485\n",
            "[132,    20] running loss:10.85075,  box loss: 0.01084, class loss:1.07516\n",
            "[133,    10] running loss:10.85537,  box loss: 0.00513, class loss:1.04361\n",
            "[133,    20] running loss:10.91552,  box loss: 0.01052, class loss:1.04549\n",
            "[134,    10] running loss:10.99108,  box loss: 0.00450, class loss:1.13570\n",
            "[134,    20] running loss:10.95999,  box loss: 0.00851, class loss:1.07483\n",
            "[135,    10] running loss:10.86534,  box loss: 0.00537, class loss:1.07029\n",
            "[135,    20] running loss:10.74157,  box loss: 0.00896, class loss:1.07477\n",
            "[136,    10] running loss:10.87263,  box loss: 0.00544, class loss:1.07485\n",
            "[136,    20] running loss:10.87062,  box loss: 0.00911, class loss:1.04365\n",
            "[137,    10] running loss:10.93904,  box loss: 0.00583, class loss:1.10626\n",
            "[137,    20] running loss:10.82813,  box loss: 0.00796, class loss:1.07514\n",
            "[138,    10] running loss:10.74572,  box loss: 0.00725, class loss:1.04383\n",
            "[138,    20] running loss:10.95908,  box loss: 0.00569, class loss:1.06756\n",
            "[139,    10] running loss:10.79680,  box loss: 0.00968, class loss:1.04380\n",
            "[139,    20] running loss:10.77533,  box loss: 0.00748, class loss:1.04360\n",
            "[140,    10] running loss:10.94016,  box loss: 0.00919, class loss:1.04371\n",
            "[140,    20] running loss:10.77236,  box loss: 0.01117, class loss:1.07488\n",
            "[141,    10] running loss:10.93449,  box loss: 0.00430, class loss:1.04384\n",
            "[141,    20] running loss:10.82723,  box loss: 0.00821, class loss:1.10486\n",
            "[142,    10] running loss:10.91507,  box loss: 0.00584, class loss:1.04474\n",
            "[142,    20] running loss:10.79124,  box loss: 0.00374, class loss:1.07397\n",
            "[143,    10] running loss:10.88338,  box loss: 0.00695, class loss:1.07521\n",
            "[143,    20] running loss:10.78969,  box loss: 0.00989, class loss:1.07588\n",
            "[144,    10] running loss:10.73201,  box loss: 0.00604, class loss:1.07466\n",
            "[144,    20] running loss:10.96112,  box loss: 0.00363, class loss:1.04378\n",
            "[145,    10] running loss:10.78919,  box loss: 0.01656, class loss:1.07503\n",
            "[145,    20] running loss:10.73635,  box loss: 0.00595, class loss:1.10517\n",
            "[146,    10] running loss:10.75992,  box loss: 0.01172, class loss:1.04408\n",
            "[146,    20] running loss:10.76146,  box loss: 0.00846, class loss:1.04361\n",
            "[147,    10] running loss:10.81449,  box loss: 0.01049, class loss:1.07484\n",
            "[147,    20] running loss:10.84335,  box loss: 0.00660, class loss:1.04383\n",
            "[148,    10] running loss:10.82591,  box loss: 0.00699, class loss:1.04360\n",
            "[148,    20] running loss:10.81875,  box loss: 0.00448, class loss:1.07482\n",
            "[149,    10] running loss:10.78301,  box loss: 0.00621, class loss:1.15847\n",
            "[149,    20] running loss:11.07850,  box loss: 0.00395, class loss:1.15634\n",
            "[150,    10] running loss:10.91363,  box loss: 0.00721, class loss:1.07928\n",
            "[150,    20] running loss:11.03979,  box loss: 0.00540, class loss:1.07462\n",
            "[151,    10] running loss:10.92213,  box loss: 0.00473, class loss:1.07520\n",
            "[151,    20] running loss:10.87038,  box loss: 0.00640, class loss:1.14022\n",
            "[152,    10] running loss:10.92755,  box loss: 0.00724, class loss:1.07484\n",
            "[152,    20] running loss:10.98814,  box loss: 0.00762, class loss:1.07428\n",
            "[153,    10] running loss:11.08182,  box loss: 0.00511, class loss:1.15294\n",
            "[153,    20] running loss:10.89486,  box loss: 0.00964, class loss:1.10594\n",
            "[154,    10] running loss:10.97081,  box loss: 0.00369, class loss:1.13749\n",
            "[154,    20] running loss:11.64743,  box loss: 0.00775, class loss:1.18660\n",
            "[155,    10] running loss:11.31441,  box loss: 0.00587, class loss:1.13708\n",
            "[155,    20] running loss:11.08304,  box loss: 0.00386, class loss:1.12324\n",
            "[156,    10] running loss:11.17399,  box loss: 0.00884, class loss:1.04719\n",
            "[156,    20] running loss:10.95190,  box loss: 0.00659, class loss:1.13520\n",
            "[157,    10] running loss:11.01873,  box loss: 0.00812, class loss:1.04400\n",
            "[157,    20] running loss:11.21762,  box loss: 0.00692, class loss:1.12907\n",
            "[158,    10] running loss:10.85554,  box loss: 0.00916, class loss:1.07157\n",
            "[158,    20] running loss:11.16535,  box loss: 0.00877, class loss:1.10574\n",
            "[159,    10] running loss:10.85710,  box loss: 0.00386, class loss:1.07489\n",
            "[159,    20] running loss:10.95936,  box loss: 0.00471, class loss:1.05839\n",
            "[160,    10] running loss:10.79551,  box loss: 0.01058, class loss:1.04782\n",
            "[160,    20] running loss:10.90014,  box loss: 0.00742, class loss:1.04372\n",
            "[161,    10] running loss:11.00830,  box loss: 0.00966, class loss:1.10670\n",
            "[161,    20] running loss:10.78368,  box loss: 0.00576, class loss:1.07569\n",
            "[162,    10] running loss:10.84502,  box loss: 0.00618, class loss:1.07484\n",
            "[162,    20] running loss:10.73838,  box loss: 0.00999, class loss:1.07488\n",
            "[163,    10] running loss:10.74846,  box loss: 0.00795, class loss:1.07689\n",
            "[163,    20] running loss:10.78882,  box loss: 0.01023, class loss:1.10592\n",
            "[164,    10] running loss:10.99733,  box loss: 0.00317, class loss:1.15764\n",
            "[164,    20] running loss:10.67479,  box loss: 0.00688, class loss:1.04587\n",
            "[165,    10] running loss:10.95552,  box loss: 0.00659, class loss:1.07339\n",
            "[165,    20] running loss:10.76749,  box loss: 0.00557, class loss:1.07482\n",
            "[166,    10] running loss:10.67325,  box loss: 0.00570, class loss:1.07491\n",
            "[166,    20] running loss:10.85388,  box loss: 0.00639, class loss:1.16860\n",
            "[167,    10] running loss:10.81344,  box loss: 0.00521, class loss:1.07491\n",
            "[167,    20] running loss:10.87956,  box loss: 0.00403, class loss:1.04594\n",
            "[168,    10] running loss:10.74039,  box loss: 0.00420, class loss:1.04576\n",
            "[168,    20] running loss:10.81644,  box loss: 0.00750, class loss:1.07493\n",
            "[169,    10] running loss:10.84251,  box loss: 0.00676, class loss:1.04361\n",
            "[169,    20] running loss:10.76657,  box loss: 0.00896, class loss:1.04360\n",
            "[170,    10] running loss:10.90272,  box loss: 0.00422, class loss:1.07335\n",
            "[170,    20] running loss:10.69501,  box loss: 0.00596, class loss:1.04359\n",
            "[171,    10] running loss:10.72358,  box loss: 0.00284, class loss:1.04369\n",
            "[171,    20] running loss:10.88132,  box loss: 0.01083, class loss:1.04368\n",
            "[172,    10] running loss:10.85405,  box loss: 0.00674, class loss:1.04424\n",
            "[172,    20] running loss:10.79840,  box loss: 0.00542, class loss:1.07646\n",
            "[173,    10] running loss:10.87717,  box loss: 0.00683, class loss:1.10616\n",
            "[173,    20] running loss:10.85546,  box loss: 0.00660, class loss:1.07394\n",
            "[174,    10] running loss:10.84212,  box loss: 0.00489, class loss:1.16595\n",
            "[174,    20] running loss:10.70673,  box loss: 0.00780, class loss:1.04360\n",
            "[175,    10] running loss:10.74339,  box loss: 0.00649, class loss:1.07470\n",
            "[175,    20] running loss:10.92350,  box loss: 0.00906, class loss:1.07442\n",
            "[176,    10] running loss:10.88033,  box loss: 0.00865, class loss:1.07477\n",
            "[176,    20] running loss:10.66705,  box loss: 0.01086, class loss:1.04361\n",
            "[177,    10] running loss:10.83217,  box loss: 0.01132, class loss:1.10525\n",
            "[177,    20] running loss:10.83497,  box loss: 0.00286, class loss:1.07343\n",
            "[178,    10] running loss:10.71980,  box loss: 0.01293, class loss:1.07484\n",
            "[178,    20] running loss:10.73924,  box loss: 0.00710, class loss:1.04362\n",
            "[179,    10] running loss:10.94161,  box loss: 0.00965, class loss:1.07861\n",
            "[179,    20] running loss:11.02660,  box loss: 0.00368, class loss:1.09482\n",
            "[180,    10] running loss:10.85585,  box loss: 0.00615, class loss:1.14964\n",
            "[180,    20] running loss:10.86382,  box loss: 0.00974, class loss:1.07392\n",
            "[181,    10] running loss:10.77182,  box loss: 0.00362, class loss:1.07919\n",
            "[181,    20] running loss:11.01273,  box loss: 0.00587, class loss:1.11096\n",
            "[182,    10] running loss:10.86663,  box loss: 0.00806, class loss:1.10152\n",
            "[182,    20] running loss:10.99399,  box loss: 0.00749, class loss:1.07707\n",
            "[183,    10] running loss:10.88075,  box loss: 0.00347, class loss:1.13748\n",
            "[183,    20] running loss:10.85164,  box loss: 0.00609, class loss:1.07533\n",
            "[184,    10] running loss:10.75907,  box loss: 0.00872, class loss:1.07554\n",
            "[184,    20] running loss:10.85202,  box loss: 0.00863, class loss:1.10460\n",
            "[185,    10] running loss:10.84619,  box loss: 0.00478, class loss:1.13588\n",
            "[185,    20] running loss:10.72000,  box loss: 0.00315, class loss:1.04388\n",
            "[186,    10] running loss:10.97069,  box loss: 0.00346, class loss:1.13514\n",
            "[186,    20] running loss:10.91741,  box loss: 0.00754, class loss:1.07486\n",
            "[187,    10] running loss:10.79111,  box loss: 0.00530, class loss:1.10655\n",
            "[187,    20] running loss:10.85718,  box loss: 0.00578, class loss:1.13626\n",
            "[188,    10] running loss:10.88587,  box loss: 0.00885, class loss:1.07470\n",
            "[188,    20] running loss:10.76730,  box loss: 0.00513, class loss:1.04472\n",
            "[189,    10] running loss:10.76384,  box loss: 0.00562, class loss:1.10610\n",
            "[189,    20] running loss:10.87348,  box loss: 0.00782, class loss:1.10337\n",
            "[190,    10] running loss:10.78937,  box loss: 0.00609, class loss:1.10624\n",
            "[190,    20] running loss:10.81601,  box loss: 0.00548, class loss:1.08652\n",
            "[191,    10] running loss:10.83072,  box loss: 0.00495, class loss:1.07419\n",
            "[191,    20] running loss:10.69322,  box loss: 0.00430, class loss:1.07484\n",
            "[192,    10] running loss:10.79610,  box loss: 0.00518, class loss:1.17407\n",
            "[192,    20] running loss:10.92056,  box loss: 0.00494, class loss:1.04361\n",
            "[193,    10] running loss:10.90278,  box loss: 0.00398, class loss:1.04360\n",
            "[193,    20] running loss:10.79640,  box loss: 0.00615, class loss:1.07686\n",
            "[194,    10] running loss:10.89085,  box loss: 0.00671, class loss:1.10622\n",
            "[194,    20] running loss:10.79655,  box loss: 0.00689, class loss:1.07493\n",
            "[195,    10] running loss:11.68659,  box loss: 0.00515, class loss:1.21959\n",
            "[195,    20] running loss:11.94461,  box loss: 0.00580, class loss:1.18407\n",
            "[196,    10] running loss:11.77953,  box loss: 0.00859, class loss:1.29998\n",
            "[196,    20] running loss:11.58497,  box loss: 0.00514, class loss:1.12507\n",
            "[197,    10] running loss:11.71202,  box loss: 0.00453, class loss:1.10562\n",
            "[197,    20] running loss:11.65552,  box loss: 0.01131, class loss:1.10699\n",
            "[198,    10] running loss:11.44921,  box loss: 0.00862, class loss:1.10825\n",
            "[198,    20] running loss:11.00688,  box loss: 0.00477, class loss:1.18259\n",
            "[199,    10] running loss:10.88766,  box loss: 0.00557, class loss:1.04906\n",
            "[199,    20] running loss:11.08977,  box loss: 0.00432, class loss:1.04366\n",
            "[200,    10] running loss:11.12789,  box loss: 0.00923, class loss:1.10652\n",
            "[200,    20] running loss:10.85041,  box loss: 0.00412, class loss:1.07579\n",
            "[201,    10] running loss:11.00646,  box loss: 0.00552, class loss:1.12347\n",
            "[201,    20] running loss:10.74621,  box loss: 0.00360, class loss:1.04362\n",
            "[202,    10] running loss:10.92400,  box loss: 0.00131, class loss:1.13466\n",
            "[202,    20] running loss:10.82815,  box loss: 0.00759, class loss:1.08851\n",
            "[203,    10] running loss:10.91296,  box loss: 0.00650, class loss:1.10559\n",
            "[203,    20] running loss:10.95496,  box loss: 0.00712, class loss:1.07349\n",
            "[204,    10] running loss:10.87402,  box loss: 0.00772, class loss:1.07490\n",
            "[204,    20] running loss:10.72941,  box loss: 0.00625, class loss:1.07484\n",
            "[205,    10] running loss:10.95630,  box loss: 0.00822, class loss:1.08826\n",
            "[205,    20] running loss:10.85526,  box loss: 0.00445, class loss:1.04361\n",
            "[206,    10] running loss:10.84376,  box loss: 0.00607, class loss:1.07846\n",
            "[206,    20] running loss:10.56831,  box loss: 0.00842, class loss:1.04359\n",
            "[207,    10] running loss:10.68234,  box loss: 0.00697, class loss:1.07484\n",
            "[207,    20] running loss:10.82638,  box loss: 0.00530, class loss:1.07495\n",
            "[208,    10] running loss:10.73745,  box loss: 0.00431, class loss:1.07489\n",
            "[208,    20] running loss:10.75078,  box loss: 0.00822, class loss:1.10609\n",
            "[209,    10] running loss:10.66070,  box loss: 0.00757, class loss:1.07463\n",
            "[209,    20] running loss:10.77716,  box loss: 0.00598, class loss:1.04396\n",
            "[210,    10] running loss:10.75233,  box loss: 0.00386, class loss:1.13534\n",
            "[210,    20] running loss:10.67872,  box loss: 0.00261, class loss:1.10413\n",
            "[211,    10] running loss:10.84057,  box loss: 0.00592, class loss:1.04360\n",
            "[211,    20] running loss:10.71296,  box loss: 0.00641, class loss:1.07596\n",
            "[212,    10] running loss:10.70292,  box loss: 0.00366, class loss:1.07493\n",
            "[212,    20] running loss:10.70995,  box loss: 0.00627, class loss:1.07449\n",
            "[213,    10] running loss:10.80252,  box loss: 0.00946, class loss:1.04359\n",
            "[213,    20] running loss:10.77447,  box loss: 0.01160, class loss:1.13655\n",
            "[214,    10] running loss:10.76792,  box loss: 0.00776, class loss:1.07404\n",
            "[214,    20] running loss:10.80699,  box loss: 0.00582, class loss:1.04360\n",
            "[215,    10] running loss:10.77513,  box loss: 0.00762, class loss:1.04359\n",
            "[215,    20] running loss:10.76974,  box loss: 0.00985, class loss:1.10607\n",
            "[216,    10] running loss:10.68098,  box loss: 0.00590, class loss:1.04361\n",
            "[216,    20] running loss:10.79186,  box loss: 0.00918, class loss:1.07385\n",
            "[217,    10] running loss:10.77523,  box loss: 0.00450, class loss:1.07506\n",
            "[217,    20] running loss:10.77371,  box loss: 0.00832, class loss:1.04359\n",
            "[218,    10] running loss:10.79214,  box loss: 0.00555, class loss:1.07484\n",
            "[218,    20] running loss:10.75018,  box loss: 0.00489, class loss:1.07484\n",
            "[219,    10] running loss:10.80473,  box loss: 0.00240, class loss:1.04726\n",
            "[219,    20] running loss:10.77480,  box loss: 0.00596, class loss:1.04360\n",
            "[220,    10] running loss:10.61868,  box loss: 0.01133, class loss:1.04359\n",
            "[220,    20] running loss:10.90781,  box loss: 0.00938, class loss:1.07310\n",
            "[221,    10] running loss:10.77563,  box loss: 0.00196, class loss:1.07484\n",
            "[221,    20] running loss:10.65594,  box loss: 0.00970, class loss:1.04428\n",
            "[222,    10] running loss:10.72671,  box loss: 0.00491, class loss:1.05045\n",
            "[222,    20] running loss:10.71190,  box loss: 0.00519, class loss:1.04360\n",
            "[223,    10] running loss:10.94729,  box loss: 0.00418, class loss:1.11565\n",
            "[223,    20] running loss:10.62319,  box loss: 0.01007, class loss:1.07791\n",
            "[224,    10] running loss:10.70159,  box loss: 0.00598, class loss:1.07959\n",
            "[224,    20] running loss:10.79389,  box loss: 0.00575, class loss:1.10598\n",
            "[225,    10] running loss:10.65999,  box loss: 0.00600, class loss:1.07455\n",
            "[225,    20] running loss:10.80548,  box loss: 0.00534, class loss:1.16699\n",
            "[226,    10] running loss:10.74984,  box loss: 0.00919, class loss:1.07482\n",
            "[226,    20] running loss:10.69422,  box loss: 0.00332, class loss:1.05305\n",
            "[227,    10] running loss:10.69537,  box loss: 0.00726, class loss:1.04359\n",
            "[227,    20] running loss:10.73988,  box loss: 0.00973, class loss:1.07484\n",
            "[228,    10] running loss:10.73059,  box loss: 0.00820, class loss:1.04869\n",
            "[228,    20] running loss:10.77153,  box loss: 0.00442, class loss:1.07476\n",
            "[229,    10] running loss:10.74506,  box loss: 0.01039, class loss:1.04369\n",
            "[229,    20] running loss:10.70409,  box loss: 0.00661, class loss:1.10413\n",
            "[230,    10] running loss:10.74817,  box loss: 0.00586, class loss:1.07517\n",
            "[230,    20] running loss:10.68057,  box loss: 0.00595, class loss:1.04361\n",
            "[231,    10] running loss:10.77759,  box loss: 0.00723, class loss:1.10583\n",
            "[231,    20] running loss:10.66360,  box loss: 0.00408, class loss:1.10609\n",
            "[232,    10] running loss:10.70563,  box loss: 0.00556, class loss:1.05971\n",
            "[232,    20] running loss:10.63699,  box loss: 0.00579, class loss:1.07581\n",
            "[233,    10] running loss:10.78038,  box loss: 0.00289, class loss:1.04359\n",
            "[233,    20] running loss:10.65345,  box loss: 0.00940, class loss:1.09807\n",
            "[234,    10] running loss:10.83172,  box loss: 0.00349, class loss:1.07485\n",
            "[234,    20] running loss:10.61449,  box loss: 0.00308, class loss:1.05647\n",
            "[235,    10] running loss:10.71988,  box loss: 0.00353, class loss:1.06893\n",
            "[235,    20] running loss:10.71893,  box loss: 0.00292, class loss:1.10438\n",
            "[236,    10] running loss:10.59497,  box loss: 0.00621, class loss:1.04360\n",
            "[236,    20] running loss:10.78903,  box loss: 0.01152, class loss:1.07463\n",
            "[237,    10] running loss:10.77513,  box loss: 0.00632, class loss:1.04359\n",
            "[237,    20] running loss:10.77802,  box loss: 0.00465, class loss:1.04359\n",
            "[238,    10] running loss:10.80821,  box loss: 0.00601, class loss:1.13633\n",
            "[238,    20] running loss:10.75047,  box loss: 0.00429, class loss:1.07525\n",
            "[239,    10] running loss:10.71188,  box loss: 0.01055, class loss:1.07484\n",
            "[239,    20] running loss:10.80209,  box loss: 0.00870, class loss:1.07472\n",
            "[240,    10] running loss:10.83148,  box loss: 0.00335, class loss:1.07464\n",
            "[240,    20] running loss:10.75566,  box loss: 0.00815, class loss:1.07479\n",
            "[241,    10] running loss:10.75233,  box loss: 0.00846, class loss:1.04359\n",
            "[241,    20] running loss:10.81669,  box loss: 0.01072, class loss:1.07819\n",
            "[242,    10] running loss:10.80681,  box loss: 0.00465, class loss:1.07486\n",
            "[242,    20] running loss:10.63475,  box loss: 0.00990, class loss:1.07484\n",
            "[243,    10] running loss:11.16847,  box loss: 0.00535, class loss:1.19073\n",
            "[243,    20] running loss:11.61083,  box loss: 0.00902, class loss:1.19501\n",
            "[244,    10] running loss:11.57380,  box loss: 0.00589, class loss:1.13037\n",
            "[244,    20] running loss:11.76928,  box loss: 0.01362, class loss:1.13178\n",
            "[245,    10] running loss:11.63074,  box loss: 0.00499, class loss:1.13063\n",
            "[245,    20] running loss:11.40588,  box loss: 0.00536, class loss:1.09790\n",
            "[246,    10] running loss:11.20362,  box loss: 0.00591, class loss:1.08502\n",
            "[246,    20] running loss:10.99923,  box loss: 0.00605, class loss:1.04818\n",
            "[247,    10] running loss:10.78450,  box loss: 0.00811, class loss:1.04418\n",
            "[247,    20] running loss:10.92550,  box loss: 0.00626, class loss:1.07487\n",
            "[248,    10] running loss:10.85140,  box loss: 0.00616, class loss:1.04360\n",
            "[248,    20] running loss:10.94357,  box loss: 0.00257, class loss:1.07397\n",
            "[249,    10] running loss:10.77878,  box loss: 0.01007, class loss:1.15500\n",
            "[249,    20] running loss:11.21954,  box loss: 0.00520, class loss:1.16834\n",
            "[250,    10] running loss:11.21754,  box loss: 0.00283, class loss:1.13702\n",
            "[250,    20] running loss:11.07347,  box loss: 0.00819, class loss:1.06944\n",
            "[251,    10] running loss:10.85276,  box loss: 0.00528, class loss:1.07495\n",
            "[251,    20] running loss:10.83089,  box loss: 0.00277, class loss:1.10638\n",
            "[252,    10] running loss:10.69461,  box loss: 0.00567, class loss:1.06070\n",
            "[252,    20] running loss:10.73291,  box loss: 0.00813, class loss:1.04363\n",
            "[253,    10] running loss:10.68232,  box loss: 0.00115, class loss:1.07451\n",
            "[253,    20] running loss:11.00918,  box loss: 0.00672, class loss:1.07486\n",
            "[254,    10] running loss:10.78659,  box loss: 0.00692, class loss:1.10606\n",
            "[254,    20] running loss:10.90664,  box loss: 0.00829, class loss:1.04359\n",
            "[255,    10] running loss:11.05195,  box loss: 0.00712, class loss:1.12658\n",
            "[255,    20] running loss:10.97912,  box loss: 0.00744, class loss:1.07516\n",
            "[256,    10] running loss:10.85178,  box loss: 0.00616, class loss:1.07564\n",
            "[256,    20] running loss:10.93469,  box loss: 0.00381, class loss:1.04422\n",
            "[257,    10] running loss:10.78161,  box loss: 0.00866, class loss:1.04687\n",
            "[257,    20] running loss:10.92558,  box loss: 0.00440, class loss:1.07530\n",
            "[258,    10] running loss:10.78214,  box loss: 0.00530, class loss:1.04359\n",
            "[258,    20] running loss:10.84682,  box loss: 0.00632, class loss:1.16296\n",
            "[259,    10] running loss:10.84156,  box loss: 0.00608, class loss:1.10609\n",
            "[259,    20] running loss:10.75242,  box loss: 0.00527, class loss:1.04359\n",
            "[260,    10] running loss:10.88362,  box loss: 0.00963, class loss:1.04359\n",
            "[260,    20] running loss:10.64566,  box loss: 0.00535, class loss:1.19822\n",
            "[261,    10] running loss:10.68889,  box loss: 0.00837, class loss:1.04359\n",
            "[261,    20] running loss:10.89243,  box loss: 0.00772, class loss:1.07637\n",
            "[262,    10] running loss:10.85657,  box loss: 0.00575, class loss:1.04517\n",
            "[262,    20] running loss:10.66041,  box loss: 0.00351, class loss:1.10612\n",
            "[263,    10] running loss:10.74532,  box loss: 0.00371, class loss:1.10531\n",
            "[263,    20] running loss:10.80640,  box loss: 0.01191, class loss:1.04379\n",
            "[264,    10] running loss:10.90540,  box loss: 0.00846, class loss:1.04361\n",
            "[264,    20] running loss:10.73370,  box loss: 0.00388, class loss:1.06536\n",
            "[265,    10] running loss:10.78311,  box loss: 0.00812, class loss:1.07495\n",
            "[265,    20] running loss:10.70355,  box loss: 0.00596, class loss:1.11959\n",
            "[266,    10] running loss:10.77679,  box loss: 0.00898, class loss:1.13409\n",
            "[266,    20] running loss:10.89478,  box loss: 0.00690, class loss:1.12874\n",
            "[267,    10] running loss:10.91214,  box loss: 0.00516, class loss:1.19813\n",
            "[267,    20] running loss:10.85329,  box loss: 0.00267, class loss:1.07469\n",
            "[268,    10] running loss:10.85463,  box loss: 0.00531, class loss:1.05575\n",
            "[268,    20] running loss:10.84596,  box loss: 0.00400, class loss:1.07484\n",
            "[269,    10] running loss:10.79366,  box loss: 0.00598, class loss:1.04361\n",
            "[269,    20] running loss:10.78538,  box loss: 0.00989, class loss:1.07485\n",
            "[270,    10] running loss:10.68704,  box loss: 0.00477, class loss:1.10393\n",
            "[270,    20] running loss:10.73525,  box loss: 0.00163, class loss:1.10505\n",
            "[271,    10] running loss:10.66346,  box loss: 0.00747, class loss:1.04360\n",
            "[271,    20] running loss:10.68557,  box loss: 0.00579, class loss:1.10751\n",
            "[272,    10] running loss:10.75646,  box loss: 0.00794, class loss:1.04359\n",
            "[272,    20] running loss:10.71386,  box loss: 0.00325, class loss:1.07366\n",
            "[273,    10] running loss:10.78122,  box loss: 0.00347, class loss:1.04363\n",
            "[273,    20] running loss:10.64691,  box loss: 0.00744, class loss:1.04360\n",
            "[274,    10] running loss:10.60040,  box loss: 0.00640, class loss:1.07515\n",
            "[274,    20] running loss:10.78064,  box loss: 0.00726, class loss:1.07483\n",
            "[275,    10] running loss:10.75792,  box loss: 0.00359, class loss:1.07489\n",
            "[275,    20] running loss:10.69375,  box loss: 0.00965, class loss:1.04359\n",
            "[276,    10] running loss:10.64459,  box loss: 0.00588, class loss:1.04359\n",
            "[276,    20] running loss:10.82184,  box loss: 0.00269, class loss:1.10260\n",
            "[277,    10] running loss:10.64987,  box loss: 0.00508, class loss:1.07485\n",
            "[277,    20] running loss:10.80777,  box loss: 0.00437, class loss:1.05103\n",
            "[278,    10] running loss:10.65758,  box loss: 0.00418, class loss:1.04365\n",
            "[278,    20] running loss:10.81740,  box loss: 0.00168, class loss:1.04359\n",
            "[279,    10] running loss:10.69855,  box loss: 0.00689, class loss:1.07484\n",
            "[279,    20] running loss:10.69068,  box loss: 0.00564, class loss:1.07484\n",
            "[280,    10] running loss:10.72643,  box loss: 0.00478, class loss:1.04359\n",
            "[280,    20] running loss:10.69593,  box loss: 0.00702, class loss:1.08461\n",
            "[281,    10] running loss:10.82981,  box loss: 0.00650, class loss:1.09146\n",
            "[281,    20] running loss:10.56685,  box loss: 0.00682, class loss:1.07470\n",
            "[282,    10] running loss:10.68501,  box loss: 0.00441, class loss:1.07501\n",
            "[282,    20] running loss:10.65488,  box loss: 0.00970, class loss:1.07483\n",
            "[283,    10] running loss:10.71730,  box loss: 0.00865, class loss:1.07484\n",
            "[283,    20] running loss:10.68182,  box loss: 0.00543, class loss:1.10609\n",
            "[284,    10] running loss:10.65084,  box loss: 0.00606, class loss:1.04359\n",
            "[284,    20] running loss:10.69704,  box loss: 0.01219, class loss:1.04364\n",
            "[285,    10] running loss:10.70087,  box loss: 0.00652, class loss:1.07492\n",
            "[285,    20] running loss:10.66896,  box loss: 0.00438, class loss:1.04359\n",
            "[286,    10] running loss:11.25160,  box loss: 0.00579, class loss:1.20228\n",
            "[286,    20] running loss:11.66058,  box loss: 0.00460, class loss:1.14362\n",
            "[287,    10] running loss:10.87141,  box loss: 0.00615, class loss:1.04901\n",
            "[287,    20] running loss:10.75620,  box loss: 0.00473, class loss:1.07372\n",
            "[288,    10] running loss:10.86705,  box loss: 0.00426, class loss:1.07412\n",
            "[288,    20] running loss:10.68526,  box loss: 0.01053, class loss:1.04376\n",
            "[289,    10] running loss:10.67218,  box loss: 0.00297, class loss:1.04360\n",
            "[289,    20] running loss:10.92100,  box loss: 0.00476, class loss:1.10609\n",
            "[290,    10] running loss:10.80997,  box loss: 0.00647, class loss:1.07297\n",
            "[290,    20] running loss:10.66642,  box loss: 0.00470, class loss:1.04359\n",
            "[291,    10] running loss:10.72571,  box loss: 0.00728, class loss:1.07485\n",
            "[291,    20] running loss:10.72332,  box loss: 0.00590, class loss:1.04364\n",
            "[292,    10] running loss:11.08377,  box loss: 0.00821, class loss:1.07356\n",
            "[292,    20] running loss:11.39729,  box loss: 0.00614, class loss:1.12400\n",
            "[293,    10] running loss:10.95792,  box loss: 0.00731, class loss:1.07512\n",
            "[293,    20] running loss:10.83755,  box loss: 0.00298, class loss:1.10526\n",
            "[294,    10] running loss:10.80311,  box loss: 0.00426, class loss:1.10772\n",
            "[294,    20] running loss:10.65528,  box loss: 0.00199, class loss:1.04712\n",
            "[295,    10] running loss:10.81001,  box loss: 0.00561, class loss:1.04702\n",
            "[295,    20] running loss:10.69204,  box loss: 0.00673, class loss:1.04359\n",
            "[296,    10] running loss:10.72937,  box loss: 0.00746, class loss:1.10151\n",
            "[296,    20] running loss:10.68916,  box loss: 0.00334, class loss:1.04426\n",
            "[297,    10] running loss:10.76647,  box loss: 0.00606, class loss:1.07349\n",
            "[297,    20] running loss:10.77965,  box loss: 0.00485, class loss:1.07451\n",
            "[298,    10] running loss:10.71456,  box loss: 0.00774, class loss:1.04360\n",
            "[298,    20] running loss:10.80389,  box loss: 0.00417, class loss:1.07515\n",
            "[299,    10] running loss:10.70142,  box loss: 0.00607, class loss:1.07484\n",
            "[299,    20] running loss:10.76952,  box loss: 0.00995, class loss:1.07484\n",
            "[300,    10] running loss:10.66729,  box loss: 0.00454, class loss:1.07424\n",
            "[300,    20] running loss:10.74158,  box loss: 0.00717, class loss:1.04361\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = next(iter(test_dataloader))\n",
        "X_test,y_test = test[0].to(device), test[1].to(device)"
      ],
      "metadata": {
        "id": "UXNlsLo1EU35"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  out = model(X_test)\n",
        "  bound_pred, label_pred = out\n",
        "  print('bound pred: ', bound_pred[0])\n",
        "  print('label pred: ', label_pred[0])\n",
        "  print('sum:', np.sum(label_pred[0].cpu().numpy()))\n",
        "  print('actual: ', y_test[0])\n",
        "  print('pred label and actual label: ', label_pred[0].cpu().numpy().argmax(), y_test[0,0])"
      ],
      "metadata": {
        "id": "LduiErUzFVue",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab45c4f6-3c74-4cd8-f7a0-32ff90b292d5"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bound pred:  tensor([0.2297, 0.4755, 0.1450, 0.2210], device='cuda:0')\n",
            "label pred:  tensor([1.0000e+00, 1.0176e-38, 2.9215e-24, 4.7505e-29, 6.9168e-20, 1.1640e-24],\n",
            "       device='cuda:0')\n",
            "sum: 1.0\n",
            "actual:  tensor([0.0000, 0.3977, 0.1055, 0.4648, 0.3221], device='cuda:0')\n",
            "pred label and actual label:  0 tensor(0., device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RUNING_LOSS = 10.77457\n",
        "CLASS_LOSS = 1.07488\n",
        "BOX_LOSS = 0.00540\n",
        "EPOCH = 300\n",
        "PATH = '/content/drive/MyDrive/Colab Notebooks/cat_breed_recognizer/cat_breed_classifier_v3.pt'\n",
        "\n",
        "torch.save({\n",
        "    'epoch':EPOCH,\n",
        "    'class_loss':CLASS_LOSS,\n",
        "    'box_loss':BOX_LOSS,\n",
        "    'loss':RUNING_LOSS,\n",
        "    'model_state_dict':model.state_dict(),\n",
        "    'optimizer_state_dict':optimizer.state_dict()\n",
        "},PATH)\n"
      ],
      "metadata": {
        "id": "YK4KIEUC3E1l"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save(PATH_TO_DATA+'X.npy',X)\n",
        "np.save(PATH_TO_DATA+'y.npy',y)"
      ],
      "metadata": {
        "id": "oB7Ly54Z5aoI"
      },
      "execution_count": 47,
      "outputs": []
    }
  ]
}